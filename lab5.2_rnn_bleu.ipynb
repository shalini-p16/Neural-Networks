{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating machine translation witht the BLEU score\n",
    "\n",
    "So far, we have evaluated our model (both for validation and test) using the same loss as training (crossentropy). However, this is not relevant or meaningfull in terms of comparing sentences.\n",
    "\n",
    "A more suitable metric is the [BLEU](https://en.wikipedia.org/wiki/BLEU) score, which compares a predicted translation to a (set of) references. It's implemented in torchtext, therefore easy to use.\n",
    "\n",
    "Below, we provide the same preprocessing and model as in the previous lab (again, we use a subset and a small model for speed, but you can change these for performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# We'll be using torchtext and spacy to do most of the pre-processing\n",
    "import spacy\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# German and English specific pipelines\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Tokenizers\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# Fields\n",
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "\n",
    "# Dataset\n",
    "train_data, valid_data, test_data = Multi30k.splits(root='data/', exts = ('.de', '.en'), fields = (SRC, TRG))\n",
    "\n",
    "# Take a subset of the dataset (for speed)\n",
    "train_data.examples = train_data.examples[:1000]\n",
    "valid_data.examples = valid_data.examples[:100]\n",
    "test_data.examples = train_data.examples[:100]\n",
    "\n",
    "# Vocabulary\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "# Dataloader (here we keep the validation dataloader)\n",
    "batch_size = 128\n",
    "train_dataloader, valid_dataloader, test_dataloader = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size = batch_size)\n",
    "\n",
    "# Fetch one example\n",
    "example_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index to string functions\n",
    "def itos_list_de(tensor_indx):\n",
    "    return [SRC.vocab.itos[tensor_indx[i]] for i in range(len(tensor_indx))]\n",
    "\n",
    "def itos_list_en(tensor_indx):\n",
    "    return [TRG.vocab.itos[tensor_indx[i]] for i in range(len(tensor_indx))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all the parameters of the network.\n",
    "input_dim = len(SRC.vocab)\n",
    "output_dim = len(TRG.vocab)\n",
    "embedding_dim_enc = 32\n",
    "embedding_dim_dec = 32\n",
    "hidden_dim_enc = 50\n",
    "hidden_dim_dec = 50\n",
    "n_layers = 1\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim_enc, hidden_dim_enc, n_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store the parameters\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim_enc = embedding_dim_enc\n",
    "        self.hidden_dim_enc = hidden_dim_enc\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Create the layers\n",
    "        self.embedding_layer = nn.Embedding(input_dim, embedding_dim_enc)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.gru = nn.GRU(embedding_dim_enc, hidden_dim_enc, n_layers)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        # Write the forward pass and return both outputs of the GRU (the set of all outputs and the context vector)\n",
    "        y = self.embedding_layer(src)\n",
    "        y = self.dropout(y)\n",
    "        enc_outputs, enc_context = self.gru(y)\n",
    "        \n",
    "        return enc_outputs, enc_context\n",
    "    \n",
    "\n",
    "# Attention module\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim_enc, hidden_dim_dec):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the energy layer and the weighted sum layer\n",
    "        self.energy_layer = nn.Sequential(nn.Linear(hidden_dim_enc + hidden_dim_dec, hidden_dim_dec), torch.nn.Tanh())\n",
    "        self.v = nn.Linear(hidden_dim_dec, 1, bias = False)\n",
    "        \n",
    "    def forward(self, dec_hidden, enc_outputs):\n",
    "        \n",
    "        src_len = enc_outputs.shape[0]\n",
    "        dec_hidden = dec_hidden.squeeze()\n",
    "        \n",
    "        # Repeat decoder hidden state src_len times\n",
    "        dec_hidden = dec_hidden.unsqueeze(0).repeat(src_len, 1, 1)\n",
    "        \n",
    "        # Permute dec_hidden and enc_outputs so that 'batch_size' is the first dimension\n",
    "        dec_hidden = dec_hidden.permute(1, 0, 2)\n",
    "        enc_outputs = enc_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        # Concatenate the inputs (hidden state of the decoder and encoder outputs)\n",
    "        # Apply the linear layer to get the energy\n",
    "        comb_inputs = torch.cat((dec_hidden, enc_outputs), dim = 2)\n",
    "        energy = self.energy_layer(comb_inputs)\n",
    "        \n",
    "        # Apply the second linear layer to get a [batch_size, src_len] attention vector\n",
    "        attn = self.v(energy).squeeze(2)\n",
    "        attn = nn.Softmax(dim=1)(attn)\n",
    "        \n",
    "        return attn\n",
    "\n",
    "# GRU decoder with attention\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim_dec, hidden_dim_enc, hidden_dim_dec, n_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store parameters\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding_dim_dec = embedding_dim_dec\n",
    "        self.hidden_dim_enc = hidden_dim_enc\n",
    "        self.hidden_dim_dec = hidden_dim_dec\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Create the layers and attention module\n",
    "        self.embedding_layer = nn.Embedding(output_dim, embedding_dim_dec)\n",
    "        self.dropout_layer = nn.Dropout(dropout_rate)\n",
    "        self.gru = nn.GRU(embedding_dim_dec + hidden_dim_enc, hidden_dim_dec, n_layers)\n",
    "        self.linear_layer = nn.Linear(hidden_dim_dec, output_dim)\n",
    "        self.attention = Attention(hidden_dim_enc, hidden_dim_dec)\n",
    "        \n",
    "    def forward(self, input_idx, input_hidden, enc_outputs):\n",
    "        \n",
    "        # Get the embeddings for the input token (same as in the previous lab)\n",
    "        y = self.dropout_layer(self.embedding_layer(input_idx))\n",
    "        y = y.unsqueeze(0)\n",
    "        \n",
    "        # Compute attention\n",
    "        a = self.attention(input_hidden, enc_outputs)\n",
    "        \n",
    "        # Compute the weighted vector\n",
    "        a = a.unsqueeze(1)\n",
    "        enc_outputs = enc_outputs.permute(1, 0, 2)\n",
    "        weighted = torch.bmm(a, enc_outputs)\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        # Concatenate the embeddings (after dropout) and the weighted vector\n",
    "        rnn_input = torch.cat((y, weighted), dim=2)\n",
    "        \n",
    "        # Apply the GRU layer\n",
    "        output, hidden = self.gru(rnn_input, input_hidden)\n",
    "        \n",
    "        # Squeeze the output of the GRU and pass it to the linear layer to have the predicted probabilites\n",
    "        output = output.squeeze(0)\n",
    "        pred_proba = self.linear_layer(output)\n",
    "        \n",
    "        return pred_proba, hidden, a\n",
    "\n",
    "# Full model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store the encoder, decoder, and the target vocabulary size\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.trg_vocab_size = decoder.output_dim\n",
    "        \n",
    "    def forward(self, src, trg_len):\n",
    "        \n",
    "        batch_size = src.shape[-1]\n",
    "        pred_probas = torch.zeros(trg_len, batch_size, self.trg_vocab_size)\n",
    "        pred_probas[0, :, 2] = 1\n",
    "        \n",
    "        src_len = src.shape[0]\n",
    "        attn = torch.zeros(batch_size, src_len, trg_len)\n",
    "        \n",
    "        enc_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        input_idx = torch.ones(batch_size).int() * 2\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            pred_proba_t, hidden, a = self.decoder(input_idx, hidden, enc_outputs)\n",
    "            pred_probas[t, :, :] = pred_proba_t\n",
    "            input_idx = pred_proba_t.argmax(-1)\n",
    "            attn[:, :, t] = a.squeeze()\n",
    "        \n",
    "        return pred_probas, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the full model and apply it to the example batch\n",
    "encoder = Encoder(input_dim, embedding_dim_enc, hidden_dim_enc, n_layers, dropout_rate)\n",
    "decoder = Decoder(output_dim, embedding_dim_dec, hidden_dim_enc, hidden_dim_dec, n_layers, dropout_rate)\n",
    "model = Seq2Seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing BLEU\n",
    "\n",
    "To compute the BLEU score, we first need to compute the set of all references and predicted sentences over the dataset. As an example, we do that hereafter on a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the source and target sentences in the example batch\n",
    "src, trg = example_batch.src, example_batch.trg\n",
    "\n",
    "# Apply the model\n",
    "trg_len = trg.shape[0]\n",
    "pred_probas, _ = model(src, trg_len)\n",
    "\n",
    "# Get the predicted target indices (of maximum proba) for the batch\n",
    "trg_pred = torch.argmax(pred_probas, -1)\n",
    "\n",
    "# Take one sentence in the batch (for both trg and trg_pred) as example\n",
    "ind_sent = 0\n",
    "trg_pred_ex = trg_pred[:, ind_sent]\n",
    "trg_ex = trg[:, ind_sent]\n",
    "\n",
    "# We need to convert the indices back to tokens.\n",
    "translation_pred = itos_list_en(trg_pred_ex)\n",
    "translation = itos_list_en(trg_ex)\n",
    "print(translation)\n",
    "print(translation_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a `cut_tokens` function which removes the useless tokens for computing the BLEU score. For instance, if the input sentence is:\n",
    "\n",
    "\\[\"\\< sos \\>\", \"Hello\", \"world\", \"!\", \"\\< eos \\>\", \"\\< pad \\>\", \"\\< pad \\>\", \"\\< pad \\>\" \\]\n",
    "\n",
    "then the output should be:\n",
    "\n",
    "\\[\"Hello\", \"world\", \"!\"\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO; write this function\n",
    "def cut_tokens(sentence):\n",
    "\n",
    "    return sentence_cut\n",
    "\n",
    "# TO DO: Apply it to to translation and translation_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how to get a clean predicted and reference sentence, we can simply loop over the sentences in the batch. To do so, we initialize empty lists `trg_all = []` and `trg_pred_all = []` to store the result. We then loop over sentences and perform the operations above until we have clean sentences. Finally, we append them to `trg_all` and `trg_pred_all`.\n",
    "\n",
    "**Note**: storing the predicted sentences is straightforward (`trg_pred_all.append(translation_pred_cut)`). However, for the references, we should append a *list of sentences* instead of a sentence. Indeed, the BLEU function expects us to provide possibly many sentences as reference translation. As a result, to append the reference we need to do: `trg_all.append([translation_cut])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the list of predictions and references\n",
    "trg_pred_all = []\n",
    "trg_all = []\n",
    "\n",
    "# TO DO: as explained above, store all the reference and predicted target sentences in the batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the BLEU score on this batch. BLEU ranges between 0 (bad) and 1 (perfect). The result on the batch should be 0 (or close to 0), because because the model has not been trained yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = bleu_score(trg_pred_all, trg_all)\n",
    "print('BLEU score on the batch: ', bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and training\n",
    "\n",
    "We can now write a function that performs evaluation by computing the BLEU score over an `eval_dataloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bleu(model, eval_dataloader, device='cpu'):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    loss_eval = 0\n",
    "\n",
    "    trg_pred_all = []\n",
    "    trg_all = []\n",
    "\n",
    "    for batch in eval_dataloader:\n",
    "\n",
    "        # Get the source and target sentence, and the target length, copy it to device\n",
    "        src, trg = batch.src.to(device), batch.trg.to(device)\n",
    "        trg_len = trg.shape[0]\n",
    "\n",
    "        # Apply the model\n",
    "        pred_probas, _ = model(src, trg_len)\n",
    "\n",
    "        # Get the predicted index (of maximum proba)\n",
    "        trg_pred = torch.argmax(pred_probas, -1)\n",
    "\n",
    "        # TO DO: using the above, get the references/predicted target sentences in the batch and append these to trg_pred_all and trg_all\n",
    "\n",
    "\n",
    "    # Get the BLEU score from the true and predicted targets\n",
    "    bleu = bleu_score(trg_pred_all, trg_all)\n",
    "    \n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training fonction with validation using the BLEU score\n",
    "# It's similar to lab4.2, but here keep in mind that BLEU should be increased (unlike the previous lab where the loss should decrease)\n",
    "\n",
    "def training_validation_bleu(model, train_dataloader, num_epochs, loss_fn, optimizer, model_name, valid_dataloader=None, device='cpu', verbose=True):\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    loss_train_total = []\n",
    "    bleu_val_total = []\n",
    "    \n",
    "    # TO DO: initialize the \"optimal\" BLEU score\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        loss_current_epoch = 0\n",
    "\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Get the source and target sentence, and the target length, copy it to device\n",
    "            src, trg = batch.src.to(device), batch.trg.to(device)\n",
    "            trg_len = trg.shape[0]\n",
    "\n",
    "            # Set the gradients at 0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Apply the model\n",
    "            pred_probas, _ = model(src, trg_len)\n",
    "\n",
    "            # Remove the first token (always <sos>) to compute the loss\n",
    "            output_dim = pred_probas.shape[-1]\n",
    "            pred_probas = pred_probas[1:]\n",
    "\n",
    "            # Reshape the pred_probas and target\n",
    "            pred_probas = pred_probas.view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss = loss_fn(pred_probas, trg)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record the loss\n",
    "            loss_current_epoch += loss.item()\n",
    "\n",
    "        # At the end of each epoch, save the average loss over batches and display it\n",
    "        loss_train_total.append(loss_current_epoch)\n",
    "        if verbose:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss_current_epoch))\n",
    "\n",
    "        # TO DO: Perform validation: save the current model only if it increases performance (i.e., decreases loss) on the validation set\n",
    "\n",
    "                \n",
    "    return loss_train_total, bleu_val_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 30\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# TO DO: Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Evaluate it on the test set in terms of BLEU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the results won't be good since we used a simple model / subset. You can use the whole dataset and a big model for obtaining a decent performance (the BLEU score on the test set should be slightly above 0.2 after 10 epochs).\n",
    "\n",
    "Feel free to play arround with these scripts : you can compare the final performance with the one from the previous script (that is, not using validation with BLEU); you can display attention as we did in 5.1, etc. Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
