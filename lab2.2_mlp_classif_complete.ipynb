{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "\n",
    "# Define the device and data repository\n",
    "device = 'cpu'\n",
    "data_dir = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/.local/lib/python3.6/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# Some widely-used datasets (like MNIST) can be downloaded directly from pytorch.\n",
    "# There are also specific commands to create a dataset object (without having to do it manually)\n",
    "\n",
    "# Choose one (or several) transform(s) to preprocess the data\n",
    "data_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                  torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# Create a Dataset (you can download the data by setting'download=True')\n",
    "train_data = torchvision.datasets.MNIST(data_dir, train=True, download=True, transform=data_transforms)\n",
    "test_data = torchvision.datasets.MNIST(data_dir, train=False, download=True, transform=data_transforms)\n",
    "num_classes = len(train_data.classes)\n",
    "\n",
    "# We are not going to work with the full dataset (which is very big), so we only keep small train and test subsets.\n",
    "train_data = Subset(train_data, torch.arange(400))\n",
    "test_data = Subset(test_data, torch.arange(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image label =  5\n"
     ]
    }
   ],
   "source": [
    "# Fetch one data pair (one image and the corresponding label)\n",
    "image, label = train_data[0]\n",
    "print(image.shape)\n",
    "plt.imshow(image[0].numpy(), cmap='gray')\n",
    "plt.show()\n",
    "print('Image label = ', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in the training subset:  50\n"
     ]
    }
   ],
   "source": [
    "# TO DO: create two dataloaders (for the training and testing subsets), with a batch_size = 8, and print the number of batches in the training subset\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "print('Number of batches in the training subset: ', len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the MLP classifier model: it has two layers and a non-linear activation function.\n",
    "\n",
    "class MLPClassif(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, act_fn):\n",
    "        super(MLPClassif, self).__init__()\n",
    "        \n",
    "        # TO DO: define the two linear layers and the activation function\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "        self.activation_fn = act_fn\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # TO DO: write the 'forward' method, which computes the output y from the input x\n",
    "        # It should apply: layer1, the activation function, layer 2, and the activation function again\n",
    "        h = self.layer1(x)\n",
    "        h = self.activation_fn(h)\n",
    "        h = self.layer2(h)\n",
    "        y = self.activation_fn(h)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:  79510\n"
     ]
    }
   ],
   "source": [
    "# To DO: Create an MLP with a hidden size of 100 and a non-linear activation function that is appropriate for classification\n",
    "# https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
    "# hint: the inputs of the model are vectorized images\n",
    "\n",
    "input_size = train_data[0][0][0].shape[0]*train_data[0][0][0].shape[1]\n",
    "hidden_size = 100\n",
    "act_fn = nn.Sigmoid()\n",
    "output_size = num_classes\n",
    "\n",
    "model = MLPClassif(input_size, hidden_size, output_size, act_fn)\n",
    "\n",
    "# A useful command to print the total number of parameters in the model\n",
    "print('Total number of parameters: ', sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function: it's very similar to what we did in lab2.1\n",
    "# The difference is that now we process batches of data instead of the whole dataset at each epoch\n",
    "\n",
    "def training_mlp_classifier(model, train_dataloader, num_epochs, loss_fn, learning_rate, device='cpu', verbose=True):\n",
    "\n",
    "    # Copy the model to the device and set it in 'training' mode (thus all gradients are computed)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize a list to record the training loss over epochs\n",
    "    loss_all_epochs = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Initialize the training loss for the current epoch\n",
    "        loss_current_epoch = 0\n",
    "        \n",
    "        # Iterate over batches using the dataloader\n",
    "        for batch_index, (images, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "            # TO DO: write the training procedure for each batch. This should consist of:\n",
    "            # - vectorizing the images (shape should be (batch_size, input_size))\n",
    "            # - copy the data (images and labels) to the device\n",
    "            # - apply the forward pass (calculate the predicted labels from the input images)\n",
    "            # - use the 'backward' method to compute the gradients\n",
    "            # - apply the gradient descent algorithm\n",
    "            # Also think of updating the loss at the current epoch\n",
    "            \n",
    "            # Prepare the inputs and labels\n",
    "            images = images.reshape(images.shape[0], -1)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            labels_predicted = model(images)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss = loss_fn(labels_predicted, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Add the batch loss to the current epoch loss\n",
    "            loss_current_epoch += loss.item()\n",
    "\n",
    "        # At the end of each epoch, record and display the loss over all batches\n",
    "        loss_all_epochs.append(loss_current_epoch)\n",
    "        if verbose:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss_current_epoch))\n",
    "        \n",
    "    return model, loss_all_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 114.9107\n",
      "Epoch [2/100], Loss: 114.3659\n",
      "Epoch [3/100], Loss: 113.8481\n",
      "Epoch [4/100], Loss: 113.3459\n",
      "Epoch [5/100], Loss: 112.8715\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3c875e81e310>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_all_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_mlp_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_mlp_classif.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8c879ec68874>\u001b[0m in \u001b[0;36mtraining_mlp_classifier\u001b[0;34m(model, train_dataloader, num_epochs, loss_fn, learning_rate, device, verbose)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Iterate over batches using the dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# TO DO: write the training procedure for each batch. This should consist of:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TO DO: Train the model (100 epochs, learning rate of 0.01) using a loss function that is appropriate for classification\n",
    "# https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "# After training, save the model parameters and display the loss over epochs\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model, loss_all_epochs = training_mlp_classifier(model, train_dataloader, num_epochs, loss_fn, learning_rate, device)\n",
    "torch.save(model.state_dict(), 'model_mlp_classif.pt')\n",
    "\n",
    "plt.plot(loss_all_epochs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function: similar to the training loop, except we don't need to compute any gradient / backprop\n",
    "\n",
    "def eval_mlp_classifier(model, eval_dataloader, device='cpu'):\n",
    "\n",
    "    # Copy the model to the device\n",
    "    model.to(device)\n",
    "    # Set the model in 'evaluation' mode (this disables some layers (batch norm, dropout...) which are not needed when testing)\n",
    "    model.eval() \n",
    "\n",
    "    # In evaluation phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        # initialize the total and correct number of labels to compute the accuracy\n",
    "        correct_labels = 0\n",
    "        total_labels = 0\n",
    "        \n",
    "        # Iterate over the dataset using the dataloader\n",
    "        for images, labels in eval_dataloader:\n",
    "\n",
    "            # Get the predicted labels classes\n",
    "            images = images.reshape(images.shape[0], -1).to(device)\n",
    "            labels = labels.to(device)\n",
    "            y_predicted = model(images)\n",
    "            \n",
    "            # To get the predicted labels, we need to get the max over all possible classes\n",
    "            _, label_predicted = torch.max(y_predicted.data, 1)\n",
    "            \n",
    "            # Compute accuracy: count the total number of samples, and the correct labels (compare the true and predicted labels)\n",
    "            total_labels += labels.size(0)\n",
    "            correct_labels += (label_predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct_labels / total_labels\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images:  84.0 %\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Evaluate the model on the test set (instanciate the model, load the trained parameters and use the\n",
    "# evaluation function). Display the accuracy on the test set.\n",
    "\n",
    "model = MLPClassif(input_size, hidden_size, num_classes, act_fn)\n",
    "model.load_state_dict(torch.load('model_mlp_classif.pt'))\n",
    "accuracy = eval_mlp_classifier(model, test_dataloader, device)\n",
    "print('Accuracy of the network on the test images: ', accuracy, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images:  48.0 %\n"
     ]
    }
   ],
   "source": [
    "# BONUS WORK: Train an MLP classifier, but use the negative log-likelihood loss\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss\n",
    "# After training, evaluate it on the test set and print the accuracy\n",
    "\n",
    "model_nll = MLPClassif(input_size, hidden_size, num_classes, act_fn)\n",
    "loss_fn = nn.NLLLoss()\n",
    "model_nll, _ = training_mlp_classifier(model_nll, train_dataloader, num_epochs, loss_fn, learning_rate, device, verbose=False)\n",
    "accuracy = eval_mlp_classifier(model_nll, test_dataloader, device)\n",
    "print('Accuracy of the network on the test images: ', accuracy, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 103.5501\n",
      "Epoch [2/100], Loss: 75.2498\n",
      "Epoch [3/100], Loss: 59.3151\n",
      "Epoch [4/100], Loss: 51.3103\n",
      "Epoch [5/100], Loss: 44.5904\n",
      "Epoch [6/100], Loss: 38.9240\n",
      "Epoch [7/100], Loss: 33.9884\n",
      "Epoch [8/100], Loss: 28.8108\n",
      "Epoch [9/100], Loss: 25.7331\n",
      "Epoch [10/100], Loss: 23.0678\n",
      "Epoch [11/100], Loss: 21.3380\n",
      "Epoch [12/100], Loss: 19.8842\n",
      "Epoch [13/100], Loss: 18.6165\n",
      "Epoch [14/100], Loss: 17.7844\n",
      "Epoch [15/100], Loss: 16.9837\n",
      "Epoch [16/100], Loss: 16.2848\n",
      "Epoch [17/100], Loss: 15.7260\n",
      "Epoch [18/100], Loss: 15.1600\n",
      "Epoch [19/100], Loss: 14.8158\n",
      "Epoch [20/100], Loss: 14.4335\n",
      "Epoch [21/100], Loss: 14.0797\n",
      "Epoch [22/100], Loss: 13.8166\n",
      "Epoch [23/100], Loss: 13.5738\n",
      "Epoch [24/100], Loss: 13.3257\n",
      "Epoch [25/100], Loss: 13.1416\n",
      "Epoch [26/100], Loss: 12.9493\n",
      "Epoch [27/100], Loss: 12.7793\n",
      "Epoch [28/100], Loss: 12.6475\n",
      "Epoch [29/100], Loss: 12.5152\n",
      "Epoch [30/100], Loss: 12.3995\n",
      "Epoch [31/100], Loss: 12.2950\n",
      "Epoch [32/100], Loss: 12.2159\n",
      "Epoch [33/100], Loss: 12.1212\n",
      "Epoch [34/100], Loss: 12.0551\n",
      "Epoch [35/100], Loss: 11.9712\n",
      "Epoch [36/100], Loss: 11.9165\n",
      "Epoch [37/100], Loss: 11.8476\n",
      "Epoch [38/100], Loss: 11.7953\n",
      "Epoch [39/100], Loss: 11.7410\n",
      "Epoch [40/100], Loss: 11.6970\n",
      "Epoch [41/100], Loss: 11.6488\n",
      "Epoch [42/100], Loss: 11.6123\n",
      "Epoch [43/100], Loss: 11.5734\n",
      "Epoch [44/100], Loss: 11.5353\n",
      "Epoch [45/100], Loss: 11.4964\n",
      "Epoch [46/100], Loss: 11.4711\n",
      "Epoch [47/100], Loss: 11.4363\n",
      "Epoch [48/100], Loss: 11.4086\n",
      "Epoch [49/100], Loss: 11.3868\n",
      "Epoch [50/100], Loss: 11.3597\n",
      "Epoch [51/100], Loss: 11.3358\n",
      "Epoch [52/100], Loss: 11.3132\n",
      "Epoch [53/100], Loss: 11.2926\n",
      "Epoch [54/100], Loss: 11.2752\n",
      "Epoch [55/100], Loss: 11.2531\n",
      "Epoch [56/100], Loss: 11.2371\n",
      "Epoch [57/100], Loss: 11.2207\n",
      "Epoch [58/100], Loss: 11.2049\n",
      "Epoch [59/100], Loss: 11.1874\n",
      "Epoch [60/100], Loss: 11.1771\n",
      "Epoch [61/100], Loss: 11.1613\n",
      "Epoch [62/100], Loss: 11.1527\n",
      "Epoch [63/100], Loss: 11.1381\n",
      "Epoch [64/100], Loss: 11.1262\n",
      "Epoch [65/100], Loss: 11.1149\n",
      "Epoch [66/100], Loss: 11.1040\n",
      "Epoch [67/100], Loss: 11.0940\n",
      "Epoch [68/100], Loss: 11.0833\n",
      "Epoch [69/100], Loss: 11.0747\n",
      "Epoch [70/100], Loss: 11.0649\n",
      "Epoch [71/100], Loss: 11.0559\n",
      "Epoch [72/100], Loss: 11.0479\n",
      "Epoch [73/100], Loss: 11.0395\n",
      "Epoch [74/100], Loss: 11.0324\n",
      "Epoch [75/100], Loss: 11.0240\n",
      "Epoch [76/100], Loss: 11.0174\n",
      "Epoch [77/100], Loss: 11.0102\n",
      "Epoch [78/100], Loss: 11.0037\n",
      "Epoch [79/100], Loss: 10.9985\n",
      "Epoch [80/100], Loss: 10.9894\n",
      "Epoch [81/100], Loss: 10.9862\n",
      "Epoch [82/100], Loss: 10.9781\n",
      "Epoch [83/100], Loss: 10.9737\n",
      "Epoch [84/100], Loss: 10.9686\n",
      "Epoch [85/100], Loss: 10.9619\n",
      "Epoch [86/100], Loss: 10.9573\n",
      "Epoch [87/100], Loss: 10.9521\n",
      "Epoch [88/100], Loss: 10.9482\n",
      "Epoch [89/100], Loss: 10.9418\n",
      "Epoch [90/100], Loss: 10.9389\n",
      "Epoch [91/100], Loss: 10.9351\n",
      "Epoch [92/100], Loss: 10.9298\n",
      "Epoch [93/100], Loss: 10.9254\n",
      "Epoch [94/100], Loss: 10.9203\n",
      "Epoch [95/100], Loss: 10.9172\n",
      "Epoch [96/100], Loss: 10.9132\n",
      "Epoch [97/100], Loss: 10.9102\n",
      "Epoch [98/100], Loss: 10.9051\n",
      "Epoch [99/100], Loss: 10.9025\n",
      "Epoch [100/100], Loss: 10.8988\n",
      "Accuracy of the network on the test images:  88.0 %\n"
     ]
    }
   ],
   "source": [
    "# BONUS WORK: Train an MLP classifier (using the binary cross entropy again) but with the RELU activation function\n",
    "# After training, evaluate it on the test set and print the accuracy\n",
    "\n",
    "model_relu = MLPClassif(input_size, hidden_size, num_classes, nn.ReLU())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model_relu, _ = training_mlp_classifier(model_relu, train_dataloader, num_epochs, loss_fn, learning_rate, device, verbose=True)\n",
    "accuracy = eval_mlp_classifier(model_relu, test_dataloader, device)\n",
    "print('Accuracy of the network on the test images: ', accuracy, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training loss: 103.0371 ; Validation accuracy: 12.0000\n",
      "Epoch [2/100], Training loss: 102.6118 ; Validation accuracy: 16.0000\n",
      "Epoch [3/100], Training loss: 102.2008 ; Validation accuracy: 22.0000\n",
      "Epoch [4/100], Training loss: 101.7991 ; Validation accuracy: 22.0000\n",
      "Epoch [5/100], Training loss: 101.4072 ; Validation accuracy: 22.0000\n",
      "Epoch [6/100], Training loss: 101.0265 ; Validation accuracy: 20.0000\n",
      "Epoch [7/100], Training loss: 100.6484 ; Validation accuracy: 26.0000\n",
      "Epoch [8/100], Training loss: 100.2705 ; Validation accuracy: 30.0000\n",
      "Epoch [9/100], Training loss: 99.8984 ; Validation accuracy: 34.0000\n",
      "Epoch [10/100], Training loss: 99.5310 ; Validation accuracy: 42.0000\n",
      "Epoch [11/100], Training loss: 99.1611 ; Validation accuracy: 48.0000\n",
      "Epoch [12/100], Training loss: 98.7825 ; Validation accuracy: 48.0000\n",
      "Epoch [13/100], Training loss: 98.4028 ; Validation accuracy: 50.0000\n",
      "Epoch [14/100], Training loss: 98.0290 ; Validation accuracy: 52.0000\n",
      "Epoch [15/100], Training loss: 97.6441 ; Validation accuracy: 52.0000\n",
      "Epoch [16/100], Training loss: 97.2653 ; Validation accuracy: 54.0000\n",
      "Epoch [17/100], Training loss: 96.8808 ; Validation accuracy: 64.0000\n",
      "Epoch [18/100], Training loss: 96.4985 ; Validation accuracy: 64.0000\n",
      "Epoch [19/100], Training loss: 96.1108 ; Validation accuracy: 64.0000\n",
      "Epoch [20/100], Training loss: 95.7255 ; Validation accuracy: 64.0000\n",
      "Epoch [21/100], Training loss: 95.3401 ; Validation accuracy: 64.0000\n",
      "Epoch [22/100], Training loss: 94.9547 ; Validation accuracy: 64.0000\n",
      "Epoch [23/100], Training loss: 94.5652 ; Validation accuracy: 68.0000\n",
      "Epoch [24/100], Training loss: 94.1861 ; Validation accuracy: 72.0000\n",
      "Epoch [25/100], Training loss: 93.8034 ; Validation accuracy: 74.0000\n",
      "Epoch [26/100], Training loss: 93.4252 ; Validation accuracy: 74.0000\n",
      "Epoch [27/100], Training loss: 93.0587 ; Validation accuracy: 76.0000\n",
      "Epoch [28/100], Training loss: 92.6807 ; Validation accuracy: 76.0000\n",
      "Epoch [29/100], Training loss: 92.3183 ; Validation accuracy: 78.0000\n",
      "Epoch [30/100], Training loss: 91.9628 ; Validation accuracy: 78.0000\n",
      "Epoch [31/100], Training loss: 91.6096 ; Validation accuracy: 78.0000\n",
      "Epoch [32/100], Training loss: 91.2603 ; Validation accuracy: 78.0000\n",
      "Epoch [33/100], Training loss: 90.9178 ; Validation accuracy: 78.0000\n",
      "Epoch [34/100], Training loss: 90.5841 ; Validation accuracy: 78.0000\n",
      "Epoch [35/100], Training loss: 90.2570 ; Validation accuracy: 78.0000\n",
      "Epoch [36/100], Training loss: 89.9371 ; Validation accuracy: 78.0000\n",
      "Epoch [37/100], Training loss: 89.6208 ; Validation accuracy: 78.0000\n",
      "Epoch [38/100], Training loss: 89.3123 ; Validation accuracy: 78.0000\n",
      "Epoch [39/100], Training loss: 89.0078 ; Validation accuracy: 78.0000\n",
      "Epoch [40/100], Training loss: 88.7174 ; Validation accuracy: 78.0000\n",
      "Epoch [41/100], Training loss: 88.4284 ; Validation accuracy: 78.0000\n",
      "Epoch [42/100], Training loss: 88.1512 ; Validation accuracy: 78.0000\n",
      "Epoch [43/100], Training loss: 87.8704 ; Validation accuracy: 78.0000\n",
      "Epoch [44/100], Training loss: 87.6056 ; Validation accuracy: 78.0000\n",
      "Epoch [45/100], Training loss: 87.3462 ; Validation accuracy: 78.0000\n",
      "Epoch [46/100], Training loss: 87.0884 ; Validation accuracy: 78.0000\n",
      "Epoch [47/100], Training loss: 86.8438 ; Validation accuracy: 78.0000\n",
      "Epoch [48/100], Training loss: 86.5964 ; Validation accuracy: 78.0000\n",
      "Epoch [49/100], Training loss: 86.3557 ; Validation accuracy: 78.0000\n",
      "Epoch [50/100], Training loss: 86.1268 ; Validation accuracy: 78.0000\n",
      "Epoch [51/100], Training loss: 85.8999 ; Validation accuracy: 80.0000\n",
      "Epoch [52/100], Training loss: 85.6799 ; Validation accuracy: 80.0000\n",
      "Epoch [53/100], Training loss: 85.4639 ; Validation accuracy: 80.0000\n",
      "Epoch [54/100], Training loss: 85.2500 ; Validation accuracy: 80.0000\n",
      "Epoch [55/100], Training loss: 85.0465 ; Validation accuracy: 80.0000\n",
      "Epoch [56/100], Training loss: 84.8445 ; Validation accuracy: 80.0000\n",
      "Epoch [57/100], Training loss: 84.6444 ; Validation accuracy: 80.0000\n",
      "Epoch [58/100], Training loss: 84.4511 ; Validation accuracy: 80.0000\n",
      "Epoch [59/100], Training loss: 84.2646 ; Validation accuracy: 80.0000\n",
      "Epoch [60/100], Training loss: 84.0795 ; Validation accuracy: 80.0000\n",
      "Epoch [61/100], Training loss: 83.8962 ; Validation accuracy: 80.0000\n",
      "Epoch [62/100], Training loss: 83.7209 ; Validation accuracy: 80.0000\n",
      "Epoch [63/100], Training loss: 83.5462 ; Validation accuracy: 80.0000\n",
      "Epoch [64/100], Training loss: 83.3763 ; Validation accuracy: 82.0000\n",
      "Epoch [65/100], Training loss: 83.2109 ; Validation accuracy: 82.0000\n",
      "Epoch [66/100], Training loss: 83.0437 ; Validation accuracy: 80.0000\n",
      "Epoch [67/100], Training loss: 82.8859 ; Validation accuracy: 82.0000\n",
      "Epoch [68/100], Training loss: 82.7282 ; Validation accuracy: 82.0000\n",
      "Epoch [69/100], Training loss: 82.5714 ; Validation accuracy: 82.0000\n",
      "Epoch [70/100], Training loss: 82.4206 ; Validation accuracy: 82.0000\n",
      "Epoch [71/100], Training loss: 82.2745 ; Validation accuracy: 82.0000\n",
      "Epoch [72/100], Training loss: 82.1262 ; Validation accuracy: 82.0000\n",
      "Epoch [73/100], Training loss: 81.9858 ; Validation accuracy: 82.0000\n",
      "Epoch [74/100], Training loss: 81.8419 ; Validation accuracy: 82.0000\n",
      "Epoch [75/100], Training loss: 81.7013 ; Validation accuracy: 82.0000\n",
      "Epoch [76/100], Training loss: 81.5654 ; Validation accuracy: 82.0000\n",
      "Epoch [77/100], Training loss: 81.4313 ; Validation accuracy: 82.0000\n",
      "Epoch [78/100], Training loss: 81.2962 ; Validation accuracy: 82.0000\n",
      "Epoch [79/100], Training loss: 81.1668 ; Validation accuracy: 82.0000\n",
      "Epoch [80/100], Training loss: 81.0404 ; Validation accuracy: 80.0000\n",
      "Epoch [81/100], Training loss: 80.9126 ; Validation accuracy: 82.0000\n",
      "Epoch [82/100], Training loss: 80.7899 ; Validation accuracy: 82.0000\n",
      "Epoch [83/100], Training loss: 80.6655 ; Validation accuracy: 82.0000\n",
      "Epoch [84/100], Training loss: 80.5448 ; Validation accuracy: 82.0000\n",
      "Epoch [85/100], Training loss: 80.4266 ; Validation accuracy: 82.0000\n",
      "Epoch [86/100], Training loss: 80.3097 ; Validation accuracy: 82.0000\n",
      "Epoch [87/100], Training loss: 80.1923 ; Validation accuracy: 82.0000\n",
      "Epoch [88/100], Training loss: 80.0773 ; Validation accuracy: 82.0000\n",
      "Epoch [89/100], Training loss: 79.9683 ; Validation accuracy: 82.0000\n",
      "Epoch [90/100], Training loss: 79.8572 ; Validation accuracy: 82.0000\n",
      "Epoch [91/100], Training loss: 79.7477 ; Validation accuracy: 82.0000\n",
      "Epoch [92/100], Training loss: 79.6416 ; Validation accuracy: 82.0000\n",
      "Epoch [93/100], Training loss: 79.5353 ; Validation accuracy: 82.0000\n",
      "Epoch [94/100], Training loss: 79.4288 ; Validation accuracy: 82.0000\n",
      "Epoch [95/100], Training loss: 79.3253 ; Validation accuracy: 82.0000\n",
      "Epoch [96/100], Training loss: 79.2232 ; Validation accuracy: 82.0000\n",
      "Epoch [97/100], Training loss: 79.1227 ; Validation accuracy: 82.0000\n",
      "Epoch [98/100], Training loss: 79.0229 ; Validation accuracy: 82.0000\n",
      "Epoch [99/100], Training loss: 78.9278 ; Validation accuracy: 82.0000\n",
      "Epoch [100/100], Training loss: 78.8298 ; Validation accuracy: 82.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BONUS WORK: Training with validation.\n",
    "# The idea is to monitor the performance of the model (when training) on another subset (called the \"validation set\").\n",
    "# This helps avoiding overfitting and allows to stop training when the performance peaks.\n",
    "# To do that, at each epoch we compute the accuracy of the model on the validation set, and check if it increases:\n",
    "# in the end, we keep the model with maximum performance\n",
    "\n",
    "# First we define the validation set by splitting the training data into 2 subsets (90% training and 10% validation)\n",
    "# We also define the corresponding dataloaders\n",
    "n_train_examples = int(len(train_data)*0.9)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "train_data, valid_data = random_split(train_data, [n_train_examples, n_valid_examples])\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# TO DO: write the training function with validation\n",
    "# - the overall training procedure is similar to what was done before\n",
    "# - at the end of each epoch, we compute the accuracy of the model on the validation subset using the evaluation function\n",
    "# - then, we check if this accuracy is increasing: if so, then we save the current model as the \"best\" or \"optimal\" model.\n",
    "# Then, perform training with validation and display the training loss and validation accuracy\n",
    "\n",
    "def train_val_mlp_classifier(model, train_dataloader, valid_dataloader, num_epochs, loss_fn, learning_rate, device='cpu'):\n",
    "\n",
    "    # First, copy the model to the device and set it in 'training' mode (thus all gradients are computed)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize a list to record the training loss and validation accuracy over epochs\n",
    "    loss_all_epochs = []\n",
    "    accuracy_val_all_epochs = []\n",
    "    \n",
    "    # Initialize the \"optimal\" accuracy, which will be used for performing validation\n",
    "    accuracy_opt = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Initialize the training loss for the current epoch\n",
    "        loss_current_epoch = 0\n",
    "        \n",
    "        # Iterate over batches (using the dataloader)\n",
    "        for batch_index, (images, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "            # Vectorize the images and copy the data to the device\n",
    "            images = images.reshape(-1, input_size)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            labels_predicted = model(images)\n",
    "            loss = loss_fn(labels_predicted, labels)\n",
    "            \n",
    "            # Gradient descent\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Add the batch loss to the current epoch loss\n",
    "            loss_current_epoch += loss.item()\n",
    "\n",
    "        # At the end of each epoch, record the loss over all batches and the accuracy on the validation set\n",
    "        loss_all_epochs.append(loss_current_epoch)\n",
    "        accuracy_current_epoch = eval_mlp_classifier(model, valid_dataloader, device)\n",
    "        accuracy_val_all_epochs.append(accuracy_current_epoch)\n",
    "        \n",
    "        # Display the training loss and validation accuracy\n",
    "        print('Epoch [{}/{}], Training loss: {:.4f} ; Validation accuracy: {:.4f}'\n",
    "               .format(epoch+1, num_epochs, loss_current_epoch, accuracy_current_epoch))\n",
    "            \n",
    "        # Now record the model if the accuracy is higher than the \"optimal\" value\n",
    "        if accuracy_current_epoch > accuracy_opt:\n",
    "            model_opt = model\n",
    "            accuracy_opt = accuracy_current_epoch\n",
    "\n",
    "    return model_opt, loss_all_epochs, accuracy_val_all_epochs\n",
    "\n",
    "# Instanciate and train an MLP classifier model\n",
    "model = MLPClassif(input_size, hidden_size, num_classes, act_fn)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "model, loss_all_epochs, accuracy_validation = train_val_mlp_classifier(model, train_dataloader, valid_dataloader, num_epochs, loss_fn, learning_rate, device)\n",
    "\n",
    "# Display the (training) loss and (validation) accuracy over epochs\n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_all_epochs)\n",
    "plt.title('Training loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracy_validation)\n",
    "plt.title('Validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
