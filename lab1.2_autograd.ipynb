{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograd (automatic differentiation) is the pytorch module that performs gradient tracking and computation\n",
    "# By default, when tensors are created their gradients is not tracked\n",
    "x = torch.ones(1, 10)\n",
    "print(x.requires_grad)\n",
    "\n",
    "# You can change it using the requires_grad_() function\n",
    "x.requires_grad_()\n",
    "print(x.requires_grad)\n",
    "\n",
    "# Alternatively, when creating a tensor, you can directly set 'requires_grad=True'\n",
    "x = torch.ones(1, 10, requires_grad=True)\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you create a tensor y from x as the result of an operation, it will have a gradient function (grad_fn) which is specific to this operation\n",
    "y = x + 50\n",
    "print(y.grad_fn)\n",
    "\n",
    "y = x * 50\n",
    "print(y.grad_fn)\n",
    "\n",
    "y = x.mean()\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward: computes the gradients \n",
    "x = torch.ones(1, 10, requires_grad=True)\n",
    "y = x.mean()\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes (for instance at testing), you don't need to keep tracking of the gradients for some operations\n",
    "# Then, in order to save memory you can simply deactivate gradient tracking\n",
    "y = x.mean()\n",
    "print(y.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x.mean()\n",
    "    print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: define a simple operation Y=W*X+B (with X=1, W=2 and B=3)\n",
    "# Compute and print the gradients of X, W and B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of creating a simple network, computing the output and the gradients\n",
    "\n",
    "# Define the network (one linear layer and a nonlinearity)\n",
    "linear_layer = nn.Linear(256, 2)\n",
    "activation_fn = nn.Sigmoid()\n",
    "print('Input size: ', linear_layer.in_features)\n",
    "print('Output size: ', linear_layer.out_features)\n",
    "print(linear_layer.bias)\n",
    "print(linear_layer.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an image-like input and an arbitrary output\n",
    "input_image = torch.randn(16,16)\n",
    "plt.imshow(input_image.numpy())\n",
    "plt.show()\n",
    "\n",
    "output_true = torch.tensor([0, 1], dtype=torch.float)\n",
    "print(output_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "input_reshape = input_image.reshape(256) # vectorize the input image\n",
    "output_predicted = activation_fn(linear_layer(input_reshape))\n",
    "print(output_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we need to define a loss function to measure the difference between the 'true' and 'predicted' output\n",
    "# This loss function will be use to compute the gradients and update the network parameters\n",
    "\n",
    "# Use the binary cross entropy loss function\n",
    "loss_fn = nn.BCELoss() \n",
    "\n",
    "# calculate the loss with the given values (true and predicted)\n",
    "loss = loss_fn(output_predicted, output_true)\n",
    "\n",
    "# You can print the loss value (or store it, which is useful for monitoring the training)\n",
    "print(loss.item())\n",
    "\n",
    "# Compute the gradients\n",
    "loss.backward()\n",
    "\n",
    "print ('Weight gradient: ', linear_layer.weight.grad)\n",
    "print ('Biases gradient: ', linear_layer.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To update the network parameters, we need to use an 'optimizer': it basically defines which optimization algorithm is used\n",
    "\n",
    "# Let's use the stochastic gradient algorithm\n",
    "optimizer = torch.optim.SGD(linear_layer.parameters(), lr=0.01)\n",
    "\n",
    "# and apply it to update the parameters\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load the linear layer of the model\n",
    "torch.save(linear_layer, 'fnn_model.pt')\n",
    "model = torch.load('fnn_model.pt')\n",
    "model\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load only the model's parameters (recommended)\n",
    "torch.save(linear_layer.state_dict(), 'fnn_model_params.pt')\n",
    "model = nn.Linear(256, 2) #need to first instanciate the model\n",
    "model.load_state_dict(torch.load('fnn_model_params.pt')) #now load its parameters\n",
    "print(model.bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
