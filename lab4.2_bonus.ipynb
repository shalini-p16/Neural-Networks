{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus work - RNN for machine translation\n",
    "\n",
    "As mentionned in the previous script, we can implement several changes and see how it affects performance.\n",
    "\n",
    "- Increase the model capacity.\n",
    "- Add dropout to the recurrent units when they use more than 1 layer.\n",
    "- Use GRU instead of LSTM: most of the implementation is the same, except you don't need to handle the extra \"cell\" state.\n",
    "- Monitoring training with validation: after each epoch, compute the loss on the validation set and save the model with the best performance (= lowest loss) on this set.\n",
    "\n",
    "We will then compare the performance of the LSTM and GRU models trained with these strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "# We'll be using torchtext and spacy to do most of the pre-processing\n",
    "import spacy\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "All the pre-processing part is the same, so we simply copy the code from the previous script here (the only difference is that we no longer take a subset of the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# German and English specific pipelines\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Tokenizers\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# Fields\n",
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "\n",
    "# Dataset\n",
    "train_data, valid_data, test_data = Multi30k.splits(root='data/', exts = ('.de', '.en'), fields = (SRC, TRG))\n",
    "\n",
    "# Vocabulary\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "# Dataloader (here we keep the validation dataloader)\n",
    "batch_size = 128\n",
    "train_dataloader, valid_dataloader, test_dataloader = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index to string functions\n",
    "def itos_list_de(tensor_indx):\n",
    "    return [SRC.vocab.itos[tensor_indx[i]] for i in range(len(tensor_indx))]\n",
    "\n",
    "def itos_list_en(tensor_indx):\n",
    "    return [TRG.vocab.itos[tensor_indx[i]] for i in range(len(tensor_indx))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model with dropout\n",
    "\n",
    "**TO DO**: write the LSTM encoder, decoder, and full seq2seq model using dropout within the recurrent units.\n",
    "Check the [doc](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) to see how it's done (99% of the code can be copied from the previous script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_dim = len(SRC.vocab)\n",
    "output_dim = len(TRG.vocab)\n",
    "embedding_dim_enc = 256\n",
    "embedding_dim_dec = 256\n",
    "hidden_dim = 512 # we use the same hidden_dim for the encoder and the decoder\n",
    "n_layers = 2\n",
    "dropout_rate = 0.5\n",
    "\n",
    "# Instanciate the LSTM model\n",
    "lstm_encoder = LSTMEncoder(input_dim, embedding_dim_enc, hidden_dim, n_layers, dropout_rate)\n",
    "lstm_decoder = LSTMDecoder(output_dim, embedding_dim_dec, hidden_dim, n_layers, dropout_rate)\n",
    "lstm_model = LSTMSeq2Seq(lstm_encoder, lstm_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU model with dropout\n",
    "\n",
    "**TO DO**: write the encoder, decoder, and full seq2seq model using GRU with recurrent dropout instead of LSTM. Again, most of the previous script's code can be reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the GRU model\n",
    "gru_encoder = GRUEncoder(input_dim, embedding_dim_enc, hidden_dim, n_layers, dropout_rate)\n",
    "gru_decoder = GRUDecoder(output_dim, embedding_dim_dec, hidden_dim, n_layers, dropout_rate)\n",
    "gru_model = GRUSeq2Seq(gru_encoder, gru_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The evaluation function is the same as in the previous script\n",
    "def evaluate_seq2seq(model, eval_dataloader, loss_fn, device='cpu', verbose=True):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    loss_eval = 0\n",
    "\n",
    "    for i, batch in enumerate(eval_dataloader):\n",
    "\n",
    "        # Get the source and target sentence, and the target length, copy it to device\n",
    "        src, trg = batch.src.to(device), batch.trg.to(device)\n",
    "        trg_len = trg.shape[0]\n",
    "\n",
    "        # Apply the model\n",
    "        pred_probas = model(src, trg_len)\n",
    "\n",
    "        # Remove the first token (always <sos>) to compute the loss\n",
    "        output_dim = pred_probas.shape[-1]\n",
    "        pred_probas = pred_probas[1:]\n",
    "\n",
    "        # Reshape the pred_probas and target so that they have appropriate shapes:\n",
    "        pred_probas = pred_probas.view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(pred_probas, trg)\n",
    "\n",
    "        # Record the loss\n",
    "        loss_eval += loss.item()\n",
    "\n",
    "    return loss_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO**: write the training function that monitor the loss using the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 10\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "optimizer_lstm = optim.Adam(lstm_model.parameters())\n",
    "optimizer_gru = optim.Adam(gru_model.parameters())\n",
    "\n",
    "# TO DO: train the LSTM and GRU models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now the models are trained, we can compare them.\n",
    "\n",
    "**TO DO**: plot the training and validation losses, the number of parameters and print the loss on the test set for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
