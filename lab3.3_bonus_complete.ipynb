{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a538af0d",
   "metadata": {},
   "source": [
    "# Bonus exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da5526e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Define the device and data repository\n",
    "device = 'cpu'\n",
    "data_dir = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c57ca5a",
   "metadata": {},
   "source": [
    "### Exercise 1: MLP vs CNN\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*1Cw9nKcdKV5YQun-e4F8gQ.png\" />\n",
    "<center><a href=\"https://towardsdatascience.com/simple-introduction-to-convolutional-neural-networks-cdf8d3077bac\">Source</a></center>\n",
    "\n",
    "In lab 2 we have defined an MLP classifier, and in lab 3 we used a CNN classifier. A natural question is: how do they compare, and which one is the best? The goal of this exercise is to answer this question, in terms of number of parameters, training behavior, and accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f21cfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Fashion MNIST dataset\n",
    "train_data = datasets.FashionMNIST(data_dir, train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = datasets.FashionMNIST(data_dir, train=False, download=True, transform=transforms.ToTensor())\n",
    "num_classes = len(train_data.classes)\n",
    "\n",
    "train_data = Subset(train_data, torch.arange(500))\n",
    "test_data = Subset(test_data, torch.arange(50))\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4c216b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the MLP and CNN (with batch norm) classifiers modules (you can reuse your code)\n",
    "\n",
    "# MLP classifier\n",
    "class MLPClassif(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, act_fn):\n",
    "        super(MLPClassif, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "        self.activation_fn = act_fn\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        h = self.layer1(x)\n",
    "        h = self.activation_fn(h)\n",
    "        h = self.layer2(h)\n",
    "        y = self.activation_fn(h)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "\n",
    "# CNN classifier\n",
    "class CNNClassif_bnorm(nn.Module):\n",
    "    def __init__(self, num_channels1=16, num_channels2=32, num_classes=10):\n",
    "        super(CNNClassif_bnorm, self).__init__()\n",
    "        \n",
    "        self.cnn_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "            \n",
    "        self.cnn_layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        self.lin_layer = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.cnn_layer1(x)\n",
    "        out = self.cnn_layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.lin_layer(out)\n",
    "        \n",
    "        return out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94412244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the training function.\n",
    "# No need to write 1 function for each module, you can use the same but be careful about vectorization.\n",
    "\n",
    "# Training function\n",
    "def training_classifier(model, train_dataloader, num_epochs, loss_fn, learning_rate, is_mlp=True, device='cpu', verbose=True):\n",
    "\n",
    "    # Set the model\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # define the optimizer (SGD)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize a list to save the training loss over epochs\n",
    "    loss_total = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_current_epoch = 0\n",
    "        for batch_index, (images, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "            # vectorize images (MLP only)\n",
    "            if is_mlp:\n",
    "                images = images.reshape(images.shape[0], -1)\n",
    "            \n",
    "            # copy images and labels to the device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            y_predicted = model(images)\n",
    "            loss = loss_fn(y_predicted, labels)\n",
    "\n",
    "            # backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Record the loss\n",
    "            loss_current_epoch += loss.item()\n",
    "\n",
    "        # At the end of each epoch, save the average loss over batches and display it\n",
    "        loss_total.append(loss_current_epoch)\n",
    "        if verbose:\n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss_current_epoch))\n",
    "        \n",
    "    return model, loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1664737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the evaluation function (again, no need to write 2)\n",
    "\n",
    "def eval_classifier(model, eval_dataloader, is_mlp=True, device='cpu', verbose=True):\n",
    "\n",
    "    # Prepare the model (copy to device and disable some layers (batch norm, dropout...) when evaluating\n",
    "    model.to(device)\n",
    "    model.eval() \n",
    "\n",
    "    # In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        # initialize the total and correct number of labels to compute the accuracy\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in eval_dataloader:\n",
    "            if is_mlp:\n",
    "                images = images.reshape(images.shape[0], -1).to(device)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            y_predicted = model(images)\n",
    "            _, label_predicted = torch.max(y_predicted.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (label_predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd0463c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 145.1036\n",
      "Epoch [2/20], Loss: 144.9614\n",
      "Epoch [3/20], Loss: 144.7607\n",
      "Epoch [4/20], Loss: 144.5635\n",
      "Epoch [5/20], Loss: 144.3765\n",
      "Epoch [6/20], Loss: 144.2261\n",
      "Epoch [7/20], Loss: 144.0685\n",
      "Epoch [8/20], Loss: 143.8943\n",
      "Epoch [9/20], Loss: 143.7087\n",
      "Epoch [10/20], Loss: 143.5264\n",
      "Epoch [11/20], Loss: 143.3393\n",
      "Epoch [12/20], Loss: 143.1736\n",
      "Epoch [13/20], Loss: 142.9660\n",
      "Epoch [14/20], Loss: 142.7912\n",
      "Epoch [15/20], Loss: 142.5954\n",
      "Epoch [16/20], Loss: 142.3963\n",
      "Epoch [17/20], Loss: 142.2076\n",
      "Epoch [18/20], Loss: 141.9378\n",
      "Epoch [19/20], Loss: 141.7246\n",
      "Epoch [20/20], Loss: 141.5034\n",
      "Epoch [1/20], Loss: 95.2880\n",
      "Epoch [2/20], Loss: 39.2696\n",
      "Epoch [3/20], Loss: 23.1412\n",
      "Epoch [4/20], Loss: 15.9258\n",
      "Epoch [5/20], Loss: 7.2640\n",
      "Epoch [6/20], Loss: 3.7136\n",
      "Epoch [7/20], Loss: 2.1442\n",
      "Epoch [8/20], Loss: 1.3039\n",
      "Epoch [9/20], Loss: 0.8888\n",
      "Epoch [10/20], Loss: 0.7873\n",
      "Epoch [11/20], Loss: 0.6028\n",
      "Epoch [12/20], Loss: 0.5126\n",
      "Epoch [13/20], Loss: 0.4428\n",
      "Epoch [14/20], Loss: 0.4096\n",
      "Epoch [15/20], Loss: 0.3791\n",
      "Epoch [16/20], Loss: 0.3301\n",
      "Epoch [17/20], Loss: 0.3149\n",
      "Epoch [18/20], Loss: 0.3017\n",
      "Epoch [19/20], Loss: 0.2683\n",
      "Epoch [20/20], Loss: 0.2430\n"
     ]
    }
   ],
   "source": [
    "# TO DO: For each model (MLP and CNN):\n",
    "# - Define the parameters of the network\n",
    "# - Instantiate the model\n",
    "# - Train it (use the same optimizer, loss function and number of epochs for both)\n",
    "# - Compute accuracy on the test set.\n",
    "\n",
    "# Common parameters\n",
    "num_classes = 10\n",
    "num_epochs = 20\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.01\n",
    "\n",
    "# MLP\n",
    "input_size_mlp = train_data[0][0][0].shape[0]*train_data[0][0][0].shape[1]\n",
    "hidden_size_mlp = 100\n",
    "model_mlp = MLPClassif(input_size_mlp, hidden_size_mlp, num_classes, nn.Sigmoid())\n",
    "model_mlp, loss_total_mlp = training_classifier(model_mlp, train_dataloader, num_epochs, loss_fn, learning_rate, is_mlp=True, device=device, verbose=True)\n",
    "accuracy_mlp = eval_classifier(model_mlp, test_dataloader, is_mlp=True, device=device, verbose=True)\n",
    "\n",
    "# CNN\n",
    "num_channels1 = 16\n",
    "num_channels2 = 32\n",
    "model_cnn = CNNClassif_bnorm(num_channels1, num_channels2, num_classes)\n",
    "model_cnn, loss_total_cnn = training_classifier(model_cnn, train_dataloader, num_epochs, loss_fn, learning_rate, is_mlp=False, device=device, verbose=True)\n",
    "accuracy_cnn = eval_classifier(model_cnn, test_dataloader, is_mlp=False, device=device, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b6254bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters\n",
      "   MLP model : 79510\n",
      "   CNN model : 29034\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images\n",
      "   MLP :  40.0 %\n",
      "   CNN :  74.0 %\n"
     ]
    }
   ],
   "source": [
    "# TO DO: compare the MLP and CNN models\n",
    "# - print the number of parameters of each model\n",
    "# - plot the training loss\n",
    "# - display accuracy\n",
    "\n",
    "print('Total number of parameters')\n",
    "print('   MLP model :', sum(p.numel() for p in model_mlp.parameters()))\n",
    "print('   CNN model :', sum(p.numel() for p in model_cnn.parameters()))\n",
    "\n",
    "plt.plot(loss_total_mlp)\n",
    "plt.plot(loss_total_cnn)\n",
    "plt.legend(['MLP', 'CNN'])\n",
    "plt.show()\n",
    "\n",
    "print('Accuracy of the network on the test images')\n",
    "print('   MLP : ', accuracy_mlp, '%')\n",
    "print('   CNN : ', accuracy_cnn, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b18b87d",
   "metadata": {},
   "source": [
    "### Exercise 2: CNN autoencoder\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/772/1*ztZn098tDQsnD5J6v1eNuQ.png\" />\n",
    "<center><a href=\"https://emkademy.medium.com/1-first-step-to-generative-deep-learning-with-autoencoders-22bd41e56d18\">Source</a></center>\n",
    "\n",
    "In lab 2.3 we have defined an MLP autoencoder. Here, we propose to do the same using CNNs. CNN autoencoders are very used in image processing applications such as image denoising, compression, and generative models (image synthesis and transformation). They can also be used for transfer learning: first an autoencoder is trained to learn a latent representation of images, and then this representation can be used for other classification/regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e47bbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: create the autoencoder module (write the __init__ and forward methods)\n",
    "\n",
    "# The encoder consists of two layers, with a convolution function, a RELU and a max pooling:\n",
    "# ------ 1st layer -----\n",
    "# convolution: 16 output channels, kernel_size=3, padding=1\n",
    "# MaxPooling: kernel_size=2, stride=2\n",
    "# ------ 2nd layer -----\n",
    "# convolution: 4 output channels, kernel_size=3, padding=1\n",
    "# MaxPooling: kernel_size=2, stride=2\n",
    "\n",
    "# Then, the decoder has the same structure \"in reverse\" using transposed convolutions and RELU (no maxpooling):\n",
    "# ------ 1st layer -----\n",
    "# transposed convolution: 16 output channels, kernel_size=2, stride=2\n",
    "# ------ 2nd layer -----\n",
    "# transposed convolution: 1 output channel, kernel_size=2, stride=2\n",
    "\n",
    "class CNNAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 4, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(4, 16, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=2, stride=2),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_est = self.decoder(z)\n",
    "        return x_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a292d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: create the training function. It should resemble other training functions you have coded,\n",
    "# but here you should use the Adam optimizer instead of SGD, see: https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam\n",
    "\n",
    "# Training function\n",
    "def train_cnn_autoencoder(model, train_dataloader, num_epochs, loss_fn, learning_rate, device, verbose=True):\n",
    "\n",
    "    # Set the model\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # define the optimizer (here, let's use ADAM for a change)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize a list to save the training loss over epochs\n",
    "    loss_all_epochs = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_current_epoch = 0\n",
    "        \n",
    "        # Iterate over batches (here we don't need labels\n",
    "        for batch_index, (images, _) in enumerate(train_dataloader):\n",
    "            \n",
    "            images = images.to(device)\n",
    "            \n",
    "            # forward pass\n",
    "            images_predicted = model(images)\n",
    "            loss = loss_fn(images_predicted, images)\n",
    "\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            loss_current_epoch += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        # At the end of each epoch, save the average loss over batches and display it\n",
    "        loss_all_epochs.append(loss_current_epoch)\n",
    "        if verbose:\n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss_current_epoch))\n",
    "        \n",
    "    return model, loss_all_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6891239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Write the eval function (same as the MLP autoencoder, but without vectorization)\n",
    "\n",
    "def eval_cnn_autoencoder(model, eval_dataloader, loss_fn, device):\n",
    "\n",
    "    # Prepare the model (copy to device and disable some layers (batch norm, dropout...) when evaluating\n",
    "    model.to(device)\n",
    "    model.eval() \n",
    "    losses = []\n",
    "    # In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        # initialize the total and correct number of labels to compute the accuracy\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, _ in eval_dataloader:\n",
    "            images = images.to(device)\n",
    "            images_predicted = model(images)\n",
    "            loss = loss_fn(images_predicted, images)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    av_loss = np.mean(losses)\n",
    "    \n",
    "    return av_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4243b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 20.8751\n",
      "Epoch [2/100], Loss: 4.3725\n",
      "Epoch [3/100], Loss: 3.0910\n",
      "Epoch [4/100], Loss: 2.8573\n",
      "Epoch [5/100], Loss: 2.5379\n",
      "Epoch [6/100], Loss: 2.2597\n",
      "Epoch [7/100], Loss: 2.0477\n",
      "Epoch [8/100], Loss: 1.8711\n",
      "Epoch [9/100], Loss: 1.7311\n",
      "Epoch [10/100], Loss: 1.6366\n",
      "Epoch [11/100], Loss: 1.5698\n",
      "Epoch [12/100], Loss: 1.5102\n",
      "Epoch [13/100], Loss: 1.4559\n",
      "Epoch [14/100], Loss: 1.4061\n",
      "Epoch [15/100], Loss: 1.3649\n",
      "Epoch [16/100], Loss: 1.3328\n",
      "Epoch [17/100], Loss: 1.3063\n",
      "Epoch [18/100], Loss: 1.2855\n",
      "Epoch [19/100], Loss: 1.2604\n",
      "Epoch [20/100], Loss: 1.2392\n",
      "Epoch [21/100], Loss: 1.2221\n",
      "Epoch [22/100], Loss: 1.2050\n",
      "Epoch [23/100], Loss: 1.1822\n",
      "Epoch [24/100], Loss: 1.1657\n",
      "Epoch [25/100], Loss: 1.1491\n",
      "Epoch [26/100], Loss: 1.1302\n",
      "Epoch [27/100], Loss: 1.1144\n",
      "Epoch [28/100], Loss: 1.1017\n",
      "Epoch [29/100], Loss: 1.0913\n",
      "Epoch [30/100], Loss: 1.0801\n",
      "Epoch [31/100], Loss: 1.0696\n",
      "Epoch [32/100], Loss: 1.0595\n",
      "Epoch [33/100], Loss: 1.0489\n",
      "Epoch [34/100], Loss: 1.0384\n",
      "Epoch [35/100], Loss: 1.0323\n",
      "Epoch [36/100], Loss: 1.0252\n",
      "Epoch [37/100], Loss: 1.0155\n",
      "Epoch [38/100], Loss: 1.0101\n",
      "Epoch [39/100], Loss: 1.0010\n",
      "Epoch [40/100], Loss: 0.9955\n",
      "Epoch [41/100], Loss: 0.9910\n",
      "Epoch [42/100], Loss: 0.9838\n",
      "Epoch [43/100], Loss: 0.9784\n",
      "Epoch [44/100], Loss: 0.9731\n",
      "Epoch [45/100], Loss: 0.9657\n",
      "Epoch [46/100], Loss: 0.9611\n",
      "Epoch [47/100], Loss: 0.9559\n",
      "Epoch [48/100], Loss: 0.9551\n",
      "Epoch [49/100], Loss: 0.9471\n",
      "Epoch [50/100], Loss: 0.9438\n",
      "Epoch [51/100], Loss: 0.9416\n",
      "Epoch [52/100], Loss: 0.9385\n",
      "Epoch [53/100], Loss: 0.9322\n",
      "Epoch [54/100], Loss: 0.9318\n",
      "Epoch [55/100], Loss: 0.9308\n",
      "Epoch [56/100], Loss: 0.9211\n",
      "Epoch [57/100], Loss: 0.9209\n",
      "Epoch [58/100], Loss: 0.9193\n",
      "Epoch [59/100], Loss: 0.9151\n",
      "Epoch [60/100], Loss: 0.9166\n",
      "Epoch [61/100], Loss: 0.9114\n",
      "Epoch [62/100], Loss: 0.9066\n",
      "Epoch [63/100], Loss: 0.9027\n",
      "Epoch [64/100], Loss: 0.9030\n",
      "Epoch [65/100], Loss: 0.8979\n",
      "Epoch [66/100], Loss: 0.9000\n",
      "Epoch [67/100], Loss: 0.8974\n",
      "Epoch [68/100], Loss: 0.8943\n",
      "Epoch [69/100], Loss: 0.8973\n",
      "Epoch [70/100], Loss: 0.8896\n",
      "Epoch [71/100], Loss: 0.8888\n",
      "Epoch [72/100], Loss: 0.8911\n",
      "Epoch [73/100], Loss: 0.8884\n",
      "Epoch [74/100], Loss: 0.8861\n",
      "Epoch [75/100], Loss: 0.8837\n",
      "Epoch [76/100], Loss: 0.8820\n",
      "Epoch [77/100], Loss: 0.8809\n",
      "Epoch [78/100], Loss: 0.8834\n",
      "Epoch [79/100], Loss: 0.8811\n",
      "Epoch [80/100], Loss: 0.8768\n",
      "Epoch [81/100], Loss: 0.8736\n",
      "Epoch [82/100], Loss: 0.8731\n",
      "Epoch [83/100], Loss: 0.8743\n",
      "Epoch [84/100], Loss: 0.8723\n",
      "Epoch [85/100], Loss: 0.8737\n",
      "Epoch [86/100], Loss: 0.8723\n",
      "Epoch [87/100], Loss: 0.8720\n",
      "Epoch [88/100], Loss: 0.8694\n",
      "Epoch [89/100], Loss: 0.8685\n",
      "Epoch [90/100], Loss: 0.8657\n",
      "Epoch [91/100], Loss: 0.8679\n",
      "Epoch [92/100], Loss: 0.8616\n",
      "Epoch [93/100], Loss: 0.8628\n",
      "Epoch [94/100], Loss: 0.8636\n",
      "Epoch [95/100], Loss: 0.8621\n",
      "Epoch [96/100], Loss: 0.8576\n",
      "Epoch [97/100], Loss: 0.8567\n",
      "Epoch [98/100], Loss: 0.8564\n",
      "Epoch [99/100], Loss: 0.8593\n",
      "Epoch [100/100], Loss: 0.8537\n",
      "MSE on the test images:  0.013856719780181135\n"
     ]
    }
   ],
   "source": [
    "# TO DO:\n",
    "# - instantiate the CNN autoencoder model\n",
    "# - define the optimize parameters: 100 epochs, MSE loss function, learning_rate=0.001\n",
    "# - train the model and evaluate it on the test set\n",
    "\n",
    "model_cnn_ae = CNNAutoencoder()\n",
    "\n",
    "num_epochs = 100\n",
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 0.001\n",
    "model_cnn_ae, loss_total = train_cnn_autoencoder(model_cnn_ae, train_dataloader, num_epochs, loss_fn, learning_rate, device)\n",
    "\n",
    "av_loss = eval_cnn_autoencoder(model_cnn_ae, test_dataloader, loss_fn, device)\n",
    "print('MSE on the test images: ', av_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04008f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAADHCAYAAAAAoQhGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAgG0lEQVR4nO2debQd1XXmvw/NSExCIDQhYSQzmbEVEIMTNcYGu9uG4ECglx3A2JjukDa9SLcx6SSEmCzIIp6CYzduMNiYqWOwWY6xQ2hjNR7ACgEEFoqEkZCEZtAIaGL3H3Ueubq1t96tO713rr7fWm+9e3edOnVO1a59q87e+xyaGYQQQuTHXgPdACGEEM0hAy6EEJkiAy6EEJkiAy6EEJkiAy6EEJkiAy6EEJkiA94gJK8j+b/bXbaBuozk9GDbIyQvacdxhMgZko+T/GSwrW3342CDe2IcOMlLAVwD4HAAGwE8BOBzZrZ+AJvlQtIAzDCzRQPdFtFeSC4GMB7ATgCbAfwIwFVmtnkg2+XRST0kOQ3AywCGmdmOJut4HMDdZtaThjpij3sCJ3kNgJsB/HcA+wGYBWAqgEdJDg/2Gdq9Foo9jA+b2RgAJwA4EcDnBrY5zaF7ZGDYoww4yX0B/AWAPzKzH5nZdjNbDOBCANMAfCyVu57k35O8m+RGAJcm2d01df0BySUk15H8U5KLSZ5Vs//d6fO0NAxyCclXSK4l+Sc19ZxM8hck15NcQfLW6IfE6c87r40kLyX5M5JfTHX9huRpSb6U5Ora4RaS/4Hkv5DcmLZfX1f37vq3F8lrSb6Utj9AcmzlCyLewcxWAvgxCkMOACA5i+TP0/V8luTsmm1jSX6T5KskXyf5vZptnyK5iORrJB8mObFmm5G8kuTCVO9XSTJtm07ypyQ3JD29P8nnpN2fJbmZ5O+TnE1yGcnPklwJ4JtJ156o7VftECDJUST/JunVBpJPkBwFoK/+9an+U1P5T5Ccn/r3Y5JTa+p9P8kXUz23AmB0boP78bKk96+n8/FbJJ9L5+TWmn0PJ/l/k56vJfkdkvvXbD8p3UebSP4fkveT/HzN9v9I8plU789JHhe1synMbI/5A3AOgB0Ahjrb7gJwb/p8PYDtAM5D8SM3KsnuTtuPRvHKewaA4QBuSeXPqtm/r+w0AAbgG6me4wFsBXBU2v7vULwFDE1l5wO4uqZdBmB60J/HAXwyfb409e0yAEMAfB7AKwC+CmAEgA8A2ARgTCo/G8CxqX/HAVgF4LwG+/cZAL8EMDnV/b/6zp3+Kunj4ppzOhnAPABfTt8nAVgH4EPpGr0/fT8obf8HAPcDOADAMAC/k+RnAlgL4KR0bf4WwJw6ffoBgP0BHApgDYBz0rZ7AfxJOt5IAGdEepj0ZweKt9kRSbcvBfBEXR/f2S/p4uOpb0MAnJb27btHhtbsdy6ARQCOSvfG/wTw87RtXNLl30t9/2+pLZ8MzvP1KN+PX099/ACAtwB8D8DBqW2ra87n9HTuRwA4CMWPzZfStuEAlqT7YRiA8wFsA/D5tP3EVNcpqb+XpGs+om06NNBK3OUb5mMAVgbbbgLwaM0Fn1O3vVYJ/gw1BgvA3unC7c6AT64p/xSAi4J2XA3goejGqSv7OHY14Atrth2b9h1fI1sH4ISgri8B+GKD/ZsP4H012yegMPClH0b97VYfF6P4odyUrtVjAPZP2z4L4Nt15X+cjMAEAG8DOMCp83YAf13zfUy6NtNq9KnWMD8A4Nr0+VsAbqvV1UgPURjwbQBG1sguRWDAUfwovAngeKfuvnuk1oA/AuDymu97AXgDxXDnHwD4Zc02AliGagZ8Us32dQB+v+b7d1HzEFVX13kA/iV9/m0Ay5F8iUn2BP7NgH8NwF/W7b8A6cehHX971BAKiieTcfTH6yak7X0s3U09E2u3m9kbKJRgd6ys+fwGihsLJN9N8gckV7IYrvkrFE8YzbCq5vObqW31sr7jnkLyJyTXkNwA4Mqa4/bXv6kAHkqvhetRGPSdKBxyohrnmdk+KAzikfi3azAVwAV95zid5zNQ6OkUAK+Z2etOfRNRPBUCAKxwiK5D8WTZh6uLAP4HCmP4FMkXSH6in7avMbO3+u8igKJfIwG81GD5qQC+XNP311LbJqGsn4bd368e9fdFdJ+MJ3kfyeXp/rwbu94ny9Px+6htx1QA19Rdwylpv7awpxnwX6AYvji/VkhyDIAPongC6mN34TkrULzy9u0/CsCBTbbpawBeROHh3xfAddjNeF4buQfAwwCmmNl+KF4p+47bX/+WAvigme1f8zfSzJZ3od09iZn9FMCdKIargOIcf7vuHI82s5vStrG1Y7E1vIrCcAAASI5Gce36vTZmttLMPmVmEwF8GsDfMQhh7dul7vsWFG9rfcc+pGbbWhRDFYc3UA9Q9PHTdf0fZWY/R6GfU2qOw9rvbeavUvuOTffnx7DrfTKpz4eQqG3HUgA31vVhbzO7t12N26MMuJltQOHE/FuS55AcxiKE6QEUr2DfbrCqvwfwYRZOwuEoXtGaNbr7oAhl3EzySAD/ucl6mjnua2b2FsmTAfynmm399e/rAG7scyqRPIjkuV1qdy/zJQDvJ3k8iie9D5M8m+QQkiOT43Cyma1AMcTwdyQPSHr826mOewFcRvIEkiNQGKAnrXDW7xaSF5Ds++F+HYXhejt9XwXgXf1U8SyAY9KxR6LQGwCAmb0N4A4AXyA5MfXp1NTGNek4tfV/HcDnSB6T2rYfyQvStn9Ixzk/vU3/VwC1PxbtZB8Uw1wbSE5CEb3Wxy9QvHleRXJougdOrtn+DQBXprddkhzNInhgn3Y1bo8y4ABgZn+N4in3FhSG80kUv5TvM7OtDdbxAoA/AnAfil/hzSicFQ3tX8cfozCem1Bc8PubqKMZ/guAG0huQjHm/UDfhgb692UUT+//mPb/JQpHjWgBM1uDYhz6z8xsKQpH3nUoDNxSFMaj7579OIqx7RdRXJurUx3/BOBPUYzjrkDxxHtRg034LQBPktyM4vp+xsx+k7ZdD+CuNBRwYdD+fwVwA4B/ArAQxXhwLX+MwlH7KxRDIjcD2CsN0d0I4Gep/llm9lDafl8aungexVsyzGwtgAtQ+K3WAZgB4GcN9rEqf4HCIbwBxQ/Hg30bzGwbirf5ywGsR/F0/gOk+8TM5gL4FIBbUfwgLkLhJ2gbe2QiT7tJQzDrUQyDvDzAzWk7vd4/IdoFyScBfN3MvtmN4+1xT+DtguSHSe6dxhhvQfFksXhgW9U+er1/QrQDkr9D8pA0hHIJipDcH3Xr+DLgzXMuCofRqyhe4S6y3nqd6fX+CdEOjkAx9r8exfQcv5d8FF1BQyhCCJEpegIXQohMacmAp1C8BSzmXbi2XY0SYqCRboscaHoIheQQAP+KYp6AZShCgy42s1/vZh+N14iOYmYtJ0E1o9tDhgyxoUN3TfDdNb9jlza22r7s623H0O2eVO+OHTuwc+fOUgWtTAF5MoBFfXGiJO9D4fgKlVyITKis20OHDsXkyZN3ke21l/+C+/bbbzcki+rohXqj/T151K49qd5XX33Vr9OVNsYk7Jr3vwy7zrcAACB5Bcm5JOe2cCwhukll3Y5uRCE6ScedmGZ2m5nNNLOZnT6WEN2kVrejpy4hOkkrQyjLsevELZPRwIQ5QmRAW3R7xw5/dbD6sfI9sd5W61S9Ba08NvwKwAySh6UJjy5CMX+CELkj3RZZ0LT5N7MdJK9CMcn8EAB3pEmQhMga6bbIhZae383shwB+2Ka2CDFokG6LHJDnRQghMkUGXAghMqV116oQAiRLiRpRbLiXfRdlQXryKkklg7Xe6Fje/tGxeqHeKBOzvo6oTj2BCyFEpsiACyFEpsiACyFEpsiACyFEpsiJKUQbMLOSk2rIkCFh2XoiJ5lXR1Tvzp07+2vmoKm31ToHc73RvDhevZETs14fonJ6AhdCiEyRARdCiEyRARdCiEyRARdCiEyRARdCiExRFIoQXSZKi/aoknZfJXqim/W2yrBhw1z5G2+80fDxvTqqXIcRI0a4cm+RhjFjxrhlN23aVJJt27bNLTty5MhdviuVXgghegwZcCGEyBQZcCGEyBQZcCGEyJSWnJgkFwPYBGAngB1mNrMdjRJioKmq2yRLjqbI0VdlJXLPeTWY6/Xw6q1SZ+SY9OqI+uD1N0p59+qNnI1e26I2eMeL2rB9+/Z+jwO0Jwrl35vZ2jbUI8RgQ7otBjUaQhFCiExp1YAbgH8k+c8kr2hHg4QYJEi3xaCn1SGUM8xsOcmDATxK8kUzm1NbICm/bgCRG5V0u9VxYiGaoaUncDNbnv6vBvAQgJOdMreZ2Uw5OEVOVNXtaG5pITpJ048NJEcD2MvMNqXPHwBwQ9taJsQA0axuN5qa7S3eEEUjVEn3Hgz1evIqq9p7vPnmm6587733Lsm81PaIaBENj82bN7vyAw88sOGy9enxQJyiX39+Vq1a5ZZr5b1vPICH0oGGArjHzH7UQn1CDBak2yILmjbgZvYbAMe3sS1CDAqk2yIXFEYohBCZIgMuhBCZotgn8Q5eJEXk6Kky77PnqNm6datbdvr06SXZokWLGj7WQFJ/TqLQwiqONs/Zl1u9HpH+eGnoUZ3efOD77ruvW/att95qqF2Ar5tRe+tT3oHYMem1Yfz48Q2V1XzgQgjRY8iACyFEpsiACyFEpsiACyFEpsiACyFEpigKZZDgeZkjz7MXGTJp0iS37KmnnlqSPfLII27ZLVu27K6JTRNFnHh89KMfLcluvvnmdjanI5hZw5E5nVrlvUq9nm51YkV5wE+vf/31192y+++/f0kWRcd4qfTr1693y3rRHvPmzXPLeqvKe8cC/JXmZ8+e7ZadP39+SbZgwQK3bP21jBaU0BO4EEJkigy4EEJkigy4EEJkigy4EEJkipyYg5gq8xW/973vdeWnnHJKSTZx4kS37Fe+8pWGj1eFgw8+uCQ7++yz3bIbN27sSBs6zWBYlb7VRSWieqswbNiwhsp5c2NH+5955pluWc85unr1arfshAkTSrKov97UAZ5jE/DT+Q855BC37Jo1a0qyZcuWNVRv5GDWE7gQQmSKDLgQQmSKDLgQQmSKDLgQQmRKvwac5B0kV5N8vkY2luSjJBem/wd0tplCtB/ptsidRtzhdwK4FcC3amTXAnjMzG4ieW36/tn2N2/PwYsgiCbSnzlzZkl21FFHuWW91axnzJjhln3ooYdKstdee80tO2rUqJJsyZIlbllv5e5o4v3IK98h7kQbdXugV6X3IhWiej2islXa60UReREca9eudff39Pi0005zy1bBO150z3jnMYoc8lagj8p6kTfRYiX15+zVV191y/V7dc1sDoD6u/hcAHelz3cBOK+/eoQYbEi3Re40OwY+3sxWpM8rAfjrAgmRH9JtkQ0tJ/KYmZEMpzEjeQWAK1o9jhDdpopuV0nOEaJdNPsEvorkBABI//30JwBmdpuZzTSz8sCtEIOPpnS71SxIIZqh2ceGhwFcAuCm9P/7bWvRHoDnAPIclqNHj3b3v+CCC0qyaM5tz3Gyzz77uGU9h1kV59oxxxzjll26dGlJFs0HPQieZJvW7dxXpa9CNM1Do07Tk046yZXPmjWrJDviiCPcsl7afbT6/OTJk0uy/fbbr+F6o/vLk0dzkntznUdp9/XBA02vSk/yXgC/AHAEyWUkL0eh3O8nuRDAWem7EFkh3Ra50+/jjpldHGx6X5vbIkRXkW6L3FEmphBCZIoMuBBCZIoMuBBCZMqAu/y7TZXVuD2PelTWk0ehZY1OnH/llVe68pUrV5Zkkfd92rRpJVk0mb6Xdh/1wYtCiFa191bUjlLpR4wYUZJF0TjR8QYL27dvd+XeuasShhitUO7p4PDhw92y3n0QtdeLbokWbvDS04888siSbMOGDe7+3rl57LHH3LKebkd98BYVWbFihVPSPzdvvvmmW9aLZInuRW/xhyhyqNHFXPQELoQQmSIDLoQQmSIDLoQQmSIDLoQQmdITTswqjslI7lFlVXjPCVVlle+LLy7nlERptk8//XRJFjmVvPTddevWuWW9ub/HjRvnlvXS8as44qKU67333rski+Yvf+aZZxo+Xqcxs4Z1y9PX6Nx5dUZp1Z48TMF2zn/k3PbSxaN63/Oe95RknrMxOlfe6vHRtAtjx44tySIHr6evkcPTm788cmJ6Zas4JqNpEeqPF05d4EqFEEIMemTAhRAiU2TAhRAiU2TAhRAiU3rCiVnFMek5byKHmueEjI5VxWF52WWXlWTenMfePNqA71iMnEre4sPLly93y3qOnsh54mWVRU6wKk5mj7PPPtuVDyYnJlDWgeiaVJnz3HOIRfrqZbFGerlp06aSLJof21sQ+LDDDnPLetfVc/RFGaKePHLQVwkyqJL96jksI+eo5+CNro93HqZOneqWrV/sOMwWd6VCCCEGPTLgQgiRKTLgQgiRKTLgQgiRKY2siXkHydUkn6+RXU9yOcln0t+HOttMIdqPdFvkTiPu8DsB3ArgW3XyL5rZLW1vUaLR1a2BaunGnje6ijc7YuLEiSXZ+eef75b1IkMWLlxYknlea8CPNjjwwAPdsp73PPJoe2nsEV50Q7Ryt1c2msvbuxann356w+2qyJ1oo243utK7F4USpV9XuQ+8aI0oCsWrN4q08Obz9tLjAX+edy+6Jeqvp0PR/NpeKnxUr6dvUSp9q/YkiuapMi9+vT2J7pd+tcPM5gAoT5IhROZIt0XutDIGfhXJ59Jr6AFta5EQA490W2RBswb8awAOB3ACgBUA/iYqSPIKknNJzm3yWEJ0k6Z0u0oilxDtoikDbmarzGynmb0N4BsATt5N2dvMbKaZzWy2kUJ0i2Z1u8pUukK0i6ZS6UlOMLO+FUF/F8DzuytfS72iR08urToWq6RqH3TQQa7cS3P1HDqAP49x5BTauHFjSebN2x05ODxnlefYBPzzGKXvevWuX7/eLes5gMI5ix2HWTS/smcIvbRvADjmmGN2+f7SSy+55arQrG6bWel6V5nzPEoX986TN+UBAKxevbokO/TQQ92yp556akkWzT/vORHHjx/vlvXmlN+8eXNJFjnlovnnPaosrO3NKR45Yj0djPT1lVdeKcmmTJnilvXOQzRNxIIFC/ptE9CAASd5L4DZAMaRXAbgzwHMJnkCAAOwGMCn+6tHiMGGdFvkTr8G3MzKS8UAt3egLUJ0Fem2yB1lYgohRKbIgAshRKbIgAshRKZ0fUGHRuNlPS93FD0xevTohmSAn8YeTU7vRQtE6beehzmKQvBSbb12RWnBXru8BRYAPzU5mkx/xYoVJVmUFuy1IVo93JsS4IAD/PwYL4ogio6onz5gyZIlbrluMHTo0FIERRQR4el2pCtedFIU7eNFSEW67Z3TSZMmuWU93YqiRbxoKG+aiSiqw+tvFKHj6VA0HYR330Y66KXNRxFh3nWLzqMXlbZmzRq3bP31ic6BnsCFECJTZMCFECJTZMCFECJTZMCFECJTBnxV+rPOOsuVe46PyIF48MEHl2SRU8hzAEX1einc0RzdnlMomkPYc/R4DsAqq1tHqbaeIy1KTd+wYUNJ5p3bqnh9ixxxnjM3crrWO3mrTJ/QboYOHVpyTk6ePNkt6zmzDj/8cLes57xq9HwAsaPOI7oPWp1n3GtXlXnOo/vIu95Rvd45GzlypFvWa2/UX+9e9nQ4qsObggOQE1MIIXoeGXAhhMgUGXAhhMgUGXAhhMgUGXAhhMiUrkah7Lvvvpg1a9Yusssvv9wt++KLL5ZkXqo34C+QEEVleBEAVVZTiSI4PC935Ln20nI9T3vkzfYiOCIvtRcdE03GX79Awu7qrXLOvEiYKOXZWzwgSkmvX8AgmnqgG4wcORLTp0/fRRZNAeDpRbSoiLd4Q5XFO6LFHxYvXlySVTl/0TVpNIIjuje8tP0oCqVKdIxXR1TWi27xpqQAqt2LXtRLtGBK/ZQC0f2mJ3AhhMgUGXAhhMgUGXAhhMgUGXAhhMiURhY1ngLgWwDGo1jo9TYz+zLJsQDuBzANxeKvF5qZPyF0YsuWLXjqqad2kdU7Nfs49thjS7LTTz+9v+a+Q+SQ8ZyQ3krakdxLNwd8J2bkfKmfxxoAjjjiiJIscvR5TtAojfz4448vyZ577jm3rOfYiqY68BxpVVLZo+uzfPnyksxzUgPlKQWqpGcD7dVtMyv1KXJQeQ7LSAdPO+20kuzll192y1ZZwf4jH/lISfbCCy+4ZT2dj1a791Z6X7t2bUlW74Duw+tDlSkJvHn5Af/8nnnmmW7ZdevWNdQuoJpD2tPjyDk6c+bMXb7fc889brlGNH4HgGvM7GgAswD8IcmjAVwL4DEzmwHgsfRdiJyQbous6deAm9kKM3s6fd4EYD6ASQDOBXBXKnYXgPM61EYhOoJ0W+ROpThwktMAnAjgSQDjzawvMHslitdQb58rAFyRPjfdUCE6Sau6Hc1SKUQnaXjQkOQYAN8FcLWZ7TKYY8XgpzsAama3mdlMM5tZdYxSiG7QDt2OpiYVopM0ZFFJDkOh4N8xsweTeBXJCWn7BAC+V0KIQYx0W+RMI1EoBHA7gPlm9oWaTQ8DuATATen/9/ura+fOnSXP/A033NBwY6PX1FNOOaUke/e73+2W9bz6nuccAI477riSLFrt3hseiqIyvPRbz0s+b948d/9HH320JHvkkUfcsl5qehUefvhhV+5FIXjRBoAf+RNNSeBFp0Se+oULFzZULqKduu1FWEVDhlOmTCnJouvkrdIePe0vW7asJIvOiRchE0UGvfTSSyVZ1DcvQmbBggUlmbfIR9SucePGuWWjqTU8vHNz0kknuWW9tkX66k0pEEWhPPvssyVZFNU2Z86cXb6vWrXKLdfIGPjpAD4OYB7JZ5LsOhTK/QDJywEsAXBhA3UJMZiQbous6deAm9kTACLv4/va2xwhuod0W+SOvIpCCJEpMuBCCJEp7OZK3iQHbtlwsUdgZgOSbDBq1Cird4ZHYbNeavgrr7zilvVSw6N63/Wud5Vk0ar0nmMxqnfs2LElWeT4r3e+Ab5zNHLaetNMrFmzxi3rzZEdrfLuzTM+Y8YMt2zksPTwHNLRFAqec3TJkiVu2fqpOVasWIGtW7eWdFtP4EIIkSky4EIIkSky4EIIkSky4EIIkSky4EIIkSldXZVeiF5lx44dpeiDaPV4b9qEKCrDSwEfNWpU2IZ6DjvsMLds/TQEADB0qG8OvDTuaOV1L719v/32K8miFeG9KBKvrQAwceLEksybpgLwo1vmzp3rlvWicaLIEq+stygJEK8s71Ef3eItMgHoCVwIIbJFBlwIITJFBlwIITJFBlwIITJFqfSipxioVPoRI0ZYvVMtclp5c2lHc3G3SuTUq7I6lufcjNrr1es5LCOHaeTcbJTonG/fvr2lNlQpG51br2w0r3q9XVYqvRBC9Bgy4EIIkSky4EIIkSky4EIIkSn9GnCSU0j+hOSvSb5A8jNJfj3J5SSfSX8f6nxzhWgf0m2RO42k0u8AcI2ZPU1yHwD/TLJvWfQvmtktnWueEB2lrbpdH30QRVREEQ2NEkWOVUnV9qgSmRKV9aJevEiLKPrC60OrkSmA394oQqdKe1utt8o592hkUeMVAFakz5tIzgcwqaWjCjEIkG6L3Klk/klOA3AigCeT6CqSz5G8g6S7dhPJK0jOJenPHCPEIKBV3Y6euoToJA0n8pAcA+CnAG40swdJjgewFoAB+EsAE8zsE/3UoUQe0VGaSeRph26PGDHC6te6jIx6lcQYjypDKFV+WKLX+SrDBI0eLxpG8vpWZQglGkZqdRgmaq/X33ac83qWLVvWfCIPyWEAvgvgO2b2IACY2Soz22lmbwP4BoCTG261EIME6bbImX7HwFmMvN8OYL6ZfaFGPiGNIQLA7wJ4vjNNFKIztFO3SZaepqInMe8ps4oD0ksLj+RRvVWeqr0V3UePHt1wvR5btmxx5d5c59HT77Zt20qy6Ek7mr/cwzsPVdpbxcG7detWt2yj7W3EHX46gI8DmEfymSS7DsDFJE9A8Zq5GMCnGzqiEIMH6bbImkaiUJ4A4I0r/rD9zRGie0i3Re4oE1MIITJFBlwIITJFBlwIITJFq9IL0Sbqo0ui6IkqMd9eFMnw4cPdslFEg4cXEVGl3igWvdF6o9T0RlPxo3pbPQdRvV7Ey+7q8KgS/99ofo6ewIUQIlNkwIUQIlNkwIUQIlNkwIUQIlO6vSr9GgBL0tdxKCYM6jXUr4FjqpkdNBAHrtHtHM5Ts/Rq33Lol6vbXTXguxyYnGtmMwfk4B1E/dqz6eXz1Kt9y7lfGkIRQohMkQEXQohMGUgDftsAHruTqF97Nr18nnq1b9n2a8DGwIUQQrSGhlCEECJTum7ASZ5DcgHJRSSv7fbx20la8HY1yedrZGNJPkpyYfrvLog7mCE5heRPSP6a5AskP5Pk2fetk/SKbkuv8+lbVw04ySEAvgrggwCORrHyydHdbEObuRPAOXWyawE8ZmYzADyWvufGDgDXmNnRAGYB+MN0nXqhbx2hx3T7Tkivs6DbT+AnA1hkZr8xs20A7gNwbpfb0DbMbA6A1+rE5wK4K32+C8B53WxTOzCzFWb2dPq8CcB8AJPQA33rID2j29LrfPrWbQM+CcDSmu/LkqyXGF+zIO5KAOMHsjGtQnIagBMBPIke61ub6XXd7qlr3yt6LSdmB7EixCfbMB+SYwB8F8DVZraxdlvufRPNk/u17yW97rYBXw5gSs33yUnWS6wiOQEA0v/VA9yepiA5DIWSf8fMHkzinuhbh+h13e6Ja99ret1tA/4rADNIHkZyOICLADzc5TZ0mocBXJI+XwLg+wPYlqZgsQTK7QDmm9kXajZl37cO0uu6nf2170W97noiD8kPAfgSgCEA7jCzG7vagDZC8l4As1HMZrYKwJ8D+B6ABwAcimJ2ugvNrN4hNKgheQaA/wdgHoC+NaOuQzFemHXfOkmv6Lb0Op++KRNTCCEyRU5MIYTIFBlwIYTIFBlwIYTIFBlwIYTIFBlwIYTIFBlwIYTIFBlwIYTIFBlwIYTIlP8Pv6u/Y2D0ztwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TO DO: Vizualization of an example\n",
    "# - load an image from the test dataset\n",
    "# - pass it to the CNN autoencoder\n",
    "# - plot the original and reconstructed images\n",
    "\n",
    "# Load an image\n",
    "image_example = test_data[0][0]\n",
    "image_example_np = image_example.numpy().squeeze()\n",
    "image_example_batch = image_example.unsqueeze(0)\n",
    "\n",
    "# Pass it to the autoencoder\n",
    "reconstructed_example = model_cnn_ae(image_example_batch.to(device))\n",
    "reconstructed_example_np = reconstructed_example.detach().cpu().numpy().squeeze()\n",
    "\n",
    "# Plot both\n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image_example_np, cmap='gray')\n",
    "plt.title('Original image')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(reconstructed_example_np, cmap='gray')\n",
    "plt.title('Reconstructed image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc0cd15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
