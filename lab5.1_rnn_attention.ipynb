{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent networks with attention mechanism\n",
    "\n",
    "In the previous lab, we have implemented a seq2seq model based on the encoder-decoder structure. The information about the input (source) sentence was passed to the decoder using a single **context vector**, which was the final output of the encoder. Doing so allows to \"compress\" all the information from the input sentence, and it's then sequentially decoded and updated in the hidden states of the decoder.\n",
    "\n",
    "However, this might be limitting for the decoding part: indeed, at a given step of the decoding process, it might be preferrable to have access to *all* the hidden states from the encoder rather than a single hidden state. This would allow to know which parts of the input sentence are the most relevent to generate the current word in the output sentence.\n",
    "\n",
    "<img src=\"https://blog.floydhub.com/content/images/2019/09/Slide36.JPG\" width=\"700\"/>\n",
    "<center><a href=\"https://blog.floydhub.com/attention-mechanism/\">Source</a></center>\n",
    "\n",
    "This can be implemented using a mechanism called **attention**, which is the topic of this lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# We'll be using torchtext and spacy to do most of the pre-processing\n",
    "import spacy\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "The pre-processing is the same as in lab 4, so we re-use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# German and English specific pipelines\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Tokenizers\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# Fields\n",
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "\n",
    "# Dataset\n",
    "train_data, valid_data, test_data = Multi30k.splits(root='data/', exts = ('.de', '.en'), fields = (SRC, TRG))\n",
    "\n",
    "# Take a subset of the dataset (for speed)\n",
    "train_data.examples = train_data.examples[:1000]\n",
    "valid_data.examples = valid_data.examples[:100]\n",
    "test_data.examples = train_data.examples[:100]\n",
    "\n",
    "# Vocabulary\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "# Dataloader (here we keep the validation dataloader)\n",
    "batch_size = 128\n",
    "train_dataloader, valid_dataloader, test_dataloader = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size = batch_size)\n",
    "\n",
    "# Fetch one example\n",
    "example_batch = next(iter(train_dataloader))\n",
    "print(example_batch.src.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index to string functions\n",
    "def itos_list_de(tensor_indx):\n",
    "    return [SRC.vocab.itos[tensor_indx[i]] for i in range(len(tensor_indx))]\n",
    "\n",
    "def itos_list_en(tensor_indx):\n",
    "    return [TRG.vocab.itos[tensor_indx[i]] for i in range(len(tensor_indx))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all the parameters of the network (small model for speed)\n",
    "input_dim = len(SRC.vocab)\n",
    "output_dim = len(TRG.vocab)\n",
    "embedding_dim_enc = 32\n",
    "embedding_dim_dec = 32\n",
    "hidden_dim_enc = 50\n",
    "hidden_dim_dec = 50\n",
    "n_layers = 1\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU encoder\n",
    "\n",
    "We use a similar encoder to the previous lab, based on GRU instead of LSTM for simplicity (no need to handle the cell state). The main difference is that it needs to output the whole sequence of hidden states (not only the final hidden state / context vector) because we will use it in the attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim_enc, hidden_dim_enc, n_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TO DO: Store the parameters\n",
    "        \n",
    "        # TO DO: Create the layers\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        # TO DO: Write the forward pass and return both outputs of the GRU (the set of all outputs and the context vector)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Instanciate the encoder and print the number of parameters\n",
    "\n",
    "# TO DO: Apply the encoder to the example batch and print the shapes of the outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The attention mechanism\n",
    "\n",
    "We now implement the attention mechanism. Intuitively, the idea behind attention is to say which word of the source sentence is the most important to generate the current word at decoding.\n",
    "\n",
    "Mathematically, let's note $t'$ the current decoding step, $s_{t'-1}$ the previous hidden state of the decoder, and $H = \\{h_1, h_2,...,h_T \\}$  the set of all encoder outputs at the last layer. The attention vector $a_{t'}$ will therefore be calculated from $s_{t'-1}$ and $H$ through a set of operation with learnable parameters.\n",
    "\n",
    "### An example\n",
    "\n",
    "For the very first target token, the previous hidden state is given by $s_0$ = $z$ (= the context vector). The attention mechanism is illustrated below in this case, and we will consider it as example.\n",
    "\n",
    "<img src=\"https://github.com/bentrevett/pytorch-seq2seq/raw/49df8404d938a6edbf729876405558cc2c2b3013//assets/seq2seq9.png\" width=\"500\"/>\n",
    "<center><a href=\"https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb\">Source</a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the context vector of the encoder and treat it as the first hidden state to the decoder\n",
    "dec_hidden = enc_context.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to assemble / combine the last hidden decoder state $s_{t'-1}$ and a given encoder output $h_t$. There are many ways to do so (addition, multiplication...), so we will consider here a concatenation $c_{t} = [ h_t, s_{t'-1}]$.\n",
    "\n",
    "Instead of doing a loop over all $t$ in the source sentence, we repeat the hidden state $s_{t'-1}$ $T$ times, and simply concatenate these: $C = [ H, S]$.\n",
    "\n",
    "**Note**: this is easy to implement when the recurrent part uses 1 layer; otherwise the computation would be more involved as we'd need to consider the extra dimension corresponding to the number of layers and duplicate $H$. We leave that to further exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: compute this concatenation:\n",
    "# - unsqueeze dec_hidden to add an extra dimension (over which repeat)\n",
    "# - repeat it using the 'repeat' function (check the doc!)\n",
    "# - concatenate the features using the 'cat' function\n",
    "# - permute the dimensions so the resulting combined input has shape [batch_size, src_length, hidden_dim_enc+hidden_dim_dec]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass this combined input to a network $f$ to compute the *energy* of the corresponding tuple:\n",
    "\n",
    "$$\n",
    "e_t = f(c_t)\n",
    "$$\n",
    "\n",
    "In practice, we use a linear layer (with learnable parameters) with a tanh activation, and we set the dimensions such that $e_t$ is a vector of length `hidden_dim_dec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Implement energy calculation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each batch and sequence element, the energy vector has a length of `hidden_dim_dec`. We want to reduce it to a single scalar value, therefore we need to reduce its dimension. We do so using a weighted sum (or equivalently, a dot product):\n",
    "\n",
    "$$\n",
    "\\hat{a}_t = v . e_t\n",
    "$$\n",
    "\n",
    "where $v$ contains these (learnable) weights. As a result we have a scalar value $\\hat{a}_t$ which is (almost) the attention for word $t$. The last step is to apply a softmax function to $\\hat{a}_t$ so that every entry is between $0$ and $1$ and the sum of attentions for a all words in a sentence sums up to $1$. This yields the attention vector $a_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Compute attention: the weighted sum is simply implemented using a linear layer with output size = 1 and no bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The attention module\n",
    "\n",
    "Now, we can use what we did above on the example to write the full attention module in the general case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim_enc, hidden_dim_dec):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TO DO: define the energy layer and the weighted sum layer\n",
    "        \n",
    "    def forward(self, dec_hidden, enc_outputs):\n",
    "        \n",
    "        # TO DO: compute attention\n",
    "        \n",
    "        return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Instanciate an Attention module, and apply it to the encoder outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the decoding step, we can now use the attention vector by applying it to the encoder outputs. This results in the *weighted* vector which is the average of encoder outputs scaled by attention:\n",
    "\n",
    "$$\n",
    "w = \\sum_t a_t \\times h_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Compute the weighted vector. It's needed to expand / permute the appropriate tensors\n",
    "# Hint : to perform vector/matrix multiplication for batches, no need for a loop: use the 'torch.bmm' function.\n",
    "# Finally, permute the weighted vector so that it has shape [1, batch_size, hidden_dim_dec]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to take this weighted vector into account when using the RNN (which here is a GRU). We remind that without attention, the RNN computation is simply $s_{t'} = \\text{RNN}(y_{t'}, s_{t'-1})$. When using attention, the formula becomes:\n",
    "\n",
    "$$\n",
    "s_{t'} = \\text{RNN}([y_{t'}, w], s_{t'-1})\n",
    "$$\n",
    "\n",
    "This means we concatenate the weighted vector with the RNN input $y_{t'}$, which is the embedding after dropout.\n",
    "\n",
    "**Note**: using this concatenation changes the dimension of the RNN input. Therefore, when defining the RNN, the input dim should no longer be `embedding_dim_dec` but `embedding_dim_dec + hidden_dim_dec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim_dec, hidden_dim_enc, hidden_dim_dec, n_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store parameters\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding_dim_dec = embedding_dim_dec\n",
    "        self.hidden_dim_enc = hidden_dim_enc\n",
    "        self.hidden_dim_dec = hidden_dim_dec\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # TO DO: Create the layers and attention module\n",
    "\n",
    "        \n",
    "    def forward(self, input_idx, input_hidden, enc_outputs):\n",
    "        \n",
    "        # Get the embeddings for the input token (same as in the previous lab)\n",
    "        y = self.dropout_layer(self.embedding_layer(input_idx))\n",
    "        y = y.unsqueeze(0)\n",
    "        \n",
    "        # TO DO: Compute attention\n",
    "        \n",
    "        # TO DO: Compute the weighted vector\n",
    "        \n",
    "        # TO DO: Concatenate the embeddings (after dropout) and the weighted vector\n",
    "        \n",
    "        # TO DO: apply the GRU layer\n",
    "        \n",
    "        # TO DO: squeeze the output of the GRU and pass it to the linear layer to have the predicted probabilites\n",
    "        \n",
    "        return pred_proba, hidden, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Instanciate the decoder, print the number of parameters\n",
    "\n",
    "# Initialize an input index tensor (corresponds to <sos>)\n",
    "input_idx = torch.ones(batch_size).int() * 2\n",
    "\n",
    "# TO DO: Apply the decoder, print the shape of the outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full model\n",
    "\n",
    "The full model is the same as in the previous lab. The only difference comes from the fact that we store the attention at every step to output it (it will be used for vizualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the Seq2Seq model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Instanciate the full model, apply it to the example_batch, and print the output shapes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (with validation) and evaluation\n",
    "\n",
    "We now implement the training function with validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The evaluation function is provided, since it's the same as in the previous lab\n",
    "def evaluate_seq2seq(model, eval_dataloader, loss_fn, device='cpu', verbose=True):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    loss_eval = 0\n",
    "\n",
    "    for i, batch in enumerate(eval_dataloader):\n",
    "\n",
    "        # Get the source and target sentence, and the target length, copy it to device\n",
    "        src, trg = batch.src.to(device), batch.trg.to(device)\n",
    "        trg_len = trg.shape[0]\n",
    "\n",
    "        # Apply the model\n",
    "        pred_probas, _ = model(src, trg_len)\n",
    "\n",
    "        # Remove the first token (always <sos>) to compute the loss\n",
    "        output_dim = pred_probas.shape[-1]\n",
    "        pred_probas = pred_probas[1:]\n",
    "\n",
    "        # Reshape the pred_probas and target so that they have appropriate shapes:\n",
    "        pred_probas = pred_probas.view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(pred_probas, trg)\n",
    "\n",
    "        # Record the loss\n",
    "        loss_eval += loss.item()\n",
    "\n",
    "    return loss_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the training function, which is also the same as in the previous lab, up to validation, which is left to implement.\n",
    "\n",
    "def training_validation_seq2seq(model, train_dataloader, num_epochs, loss_fn, optimizer, model_name, valid_dataloader=None, device='cpu', verbose=True):\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    loss_train_total = []\n",
    "    loss_val_total = []\n",
    "    \n",
    "    loss_val_optim = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        loss_current_epoch = 0\n",
    "\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Get the source and target sentence, and the target length, copy it to device\n",
    "            src, trg = batch.src.to(device), batch.trg.to(device)\n",
    "            trg_len = trg.shape[0]\n",
    "\n",
    "            # Set the gradients at 0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Apply the model\n",
    "            pred_probas, _ = model(src, trg_len)\n",
    "\n",
    "            # Remove the first token (always <sos>) to compute the loss\n",
    "            output_dim = pred_probas.shape[-1]\n",
    "            pred_probas = pred_probas[1:]\n",
    "\n",
    "            # Reshape the pred_probas and target\n",
    "            pred_probas = pred_probas.view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss = loss_fn(pred_probas, trg)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record the loss\n",
    "            loss_current_epoch += loss.item()\n",
    "\n",
    "        # At the end of each epoch, save the average loss over batches and display it\n",
    "        loss_train_total.append(loss_current_epoch)\n",
    "        if verbose:\n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss_current_epoch))\n",
    "\n",
    "        # TO DO: Perform validation: save the current model only if it increases performance (i.e., decreases loss) on the validation set\n",
    "\n",
    "                \n",
    "    return loss_train_total, loss_val_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 10\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "loss_train, loss_val = training_validation_seq2seq(model, train_dataloader, num_epochs, loss_fn, optimizer, 'model_attention', valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.load_state_dict(torch.load('model_attention.pt'))\n",
    "loss_test = evaluate_seq2seq(model, test_dataloader, loss_fn)\n",
    "print('Test loss: ', loss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some examples (source and target) in the test set\n",
    "example_batch_src, example_batch_trg = example_batch.src, example_batch.trg\n",
    "\n",
    "# Compute predictions with the model\n",
    "example_batch_trg_pred, a = model(example_batch_src, len(example_batch_trg))\n",
    "indx_pred = torch.argmax(example_batch_trg_pred, -1)\n",
    "\n",
    "# Print the true and predicted target sentences\n",
    "indx_sentence_print = 1\n",
    "\n",
    "sentence = itos_list_de(example_batch_src[:, indx_sentence_print])\n",
    "translation = itos_list_en(example_batch_trg[:, indx_sentence_print])\n",
    "translation_pred = itos_list_en(indx_pred[:, indx_sentence_print])\n",
    "attention = a[indx_sentence_print]\n",
    "\n",
    "print(sentence)\n",
    "print(translation)\n",
    "print(translation_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualizing attention\n",
    "\n",
    "We provide here a function to vizuale attention, that is, how \"strongly\" words in the source sentence relate to the words in the target sentence.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2000/1*FP3zFjdFhNUWEJ9hxeIYOA.png\" width=\"800\"/>\n",
    "<center><a href=\"https://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b\">Source</a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizualize_attention(attention, sentence, translation):\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    attention = attention.squeeze(1).detach().numpy()\n",
    "\n",
    "    y_ticks = [] \n",
    "    for t in translation:\n",
    "        y_ticks.append(t)\n",
    "        if t == '<eos>':\n",
    "            break\n",
    "\n",
    "    x_ticks = [] \n",
    "    for t in sentence:\n",
    "        x_ticks.append(t)\n",
    "        if t == '<eos>':\n",
    "            break\n",
    "\n",
    "    x_ticks = [''] + x_ticks\n",
    "\n",
    "    attention = attention[1:len(y_ticks), :len(x_ticks)-1]\n",
    "\n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "    ax.tick_params(labelsize=15)\n",
    "\n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return\n",
    "\n",
    "vizualize_attention(attention, sentence, translation_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the results are no good. As a first \"bonus\" work (do it at home because it takes time), you can:\n",
    "\n",
    "- use the whole dataset instead of a subset\n",
    "- increase the model capacity: use embedding dimensions of 256 and hidden dimensions of 512\n",
    "\n",
    "Compare the test loss of this model (GRU with attention) with the one from the previous bonus lab (GRU without attention, and with 1 layer and no recurrent dropout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
