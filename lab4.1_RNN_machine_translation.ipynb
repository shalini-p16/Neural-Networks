{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks for machine translation\n",
    "\n",
    "In this script, we work on sequence to sequence (seq2seq) learning using recurrent neural networks (it's one example of \"many-to-many\" RNNs seen in the course). This task consists in producing one sequence of data from another (of possibly different lengths).\n",
    "\n",
    "More specifically, we will work with textual data for the *machine translation* task: the goal is to automatically translate a sentence from one language to another.\n",
    "\n",
    "<img src=\"https://pytorch.org/tutorials/_images/seq2seq.png\" width=\"500\"/>\n",
    "<center><a href=\"https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\">Source</a></center>\n",
    "\n",
    "**Note**: This notebook is based on [this tutorial](https://github.com/bentrevett/pytorch-seq2seq), which you are strongly encouraged to check as it goes into much more details about seq2seq models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "# We'll be using torchtext and spacy to do most of the pre-processing\n",
    "import spacy\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing: preparing the data\n",
    "\n",
    "### Tokenizers\n",
    "\n",
    "The first step is to define *tokenizers*, that is, how a string is transformed into a sequence of words (or *tokens*). For instance, \"Welcome to the U.S.A.!\" should be transformed into \\[\"Welcome\", \"to\", \"the\", \"U.S.A.\", \"!\"\\]. These are called tokens in the general case because \"!\" is not a word.\n",
    "\n",
    "We will also use language models, in order to preserve some specific rules for tokenization: with a na√Øve approach, \"U.S.A.\" would be split into 6 tokens \\[\"U\", \".\", \"S', \".\", \"A\", \".\"\\], but we want to consider it as a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the German and English specific pipelines\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# define tokenizers\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# check on an example\n",
    "print('Welcome to the U.S.A!')\n",
    "print(tokenize_en('Welcome to the U.S.A.!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also define 'Fields', which handle how the data will be processed. In addition to tokenizing, it can convert\n",
    "# all characters to lower case and add extra tokens for the start and end of sentences.\n",
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We use the Multi30k dataset, which contains sentences in German and English (as well as French, but it's not available using torchtext)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full dataset\n",
    "train_data, valid_data, test_data = Multi30k.splits(root='data/', exts = ('.de', '.en'), fields = (SRC, TRG))\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "\n",
    "# We take a subset of the full dataset for speed\n",
    "train_data.examples = train_data.examples[:1000]\n",
    "valid_data.examples = valid_data.examples[:100]\n",
    "test_data.examples = train_data.examples[:100]\n",
    "\n",
    "# Print one example\n",
    "print(train_data.examples[0].src)\n",
    "print(train_data.examples[0].trg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "We now use the ```build_vocab``` method of the ```Field``` to create a vocabulary from the data. A vocabulary maps each token to an integer, and using the Field also transforms it into a torch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the 'vocab.itos' method to convert indices to the corresponding tokens\n",
    "# The first tokens in the vocabulary are 'unknown word', 'padding', 'start of sentence' and 'end of sequence'.\n",
    "# Then the tokens are ranked by frequency of appearence in the dataset.\n",
    "for i in range(20):\n",
    "    print(TRG.vocab.itos[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "\n",
    "The final step is to create a dataloader to generate batches of data. Instead of using the classic ```Dataloader``` function, we use ```BucketIterator```, which creates batches by assembling sentences of similar lengths This reduces the amount of padding and therefore of useless calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataloader, _, test_dataloader = BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                                                           batch_size = batch_size)\n",
    "\n",
    "# Fetch one batch as an example\n",
    "example_batch = next(iter(train_dataloader))\n",
    "\n",
    "# Each batch contains a 'src' and 'trg' entries (source and target), corresponding to English and German sentences.\n",
    "print(example_batch.src[:,1])\n",
    "\n",
    "# The shape of this tensor should be [seq_length, batch_size] where seq_length is the maximum length of a sentence in this batch\n",
    "print(example_batch.src.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write a function that takes a list of integers (such as a slice of 'example_batch' above) as input\n",
    "# and return the corresponding tokens (hint: use the 'vocab.itos' method), for both English and German\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent networks basics\n",
    "\n",
    "Now that the data is ready, let's see the basic operations used in RNNs: [embedding layers](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html?highlight=embedding#torch.nn.Embedding), [dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html), and [recurrent layers](https://pytorch.org/docs/stable/nn.html#recurrent-layers).\n",
    "\n",
    "### Embedding layer\n",
    "\n",
    "Sentences have been tokenized and tokens have been transformed into integers. We need to further transform these integers into word vectors: the idea is that two *similar* words should have similar word vectors. \n",
    "\n",
    "<img src=\"https://ruder.io/content/images/size/w2000/2016/04/word_embeddings_colah.png\" width=\"500\"/>\n",
    "<center><a href=\"https://ruder.io/word-embeddings-1/\">Source</a></center>\n",
    "\n",
    "This notion of similarity (and what word vectors exactly represent) is hard to define explicitly. Then, we use an embedding layer to produce these word vectors, and this layer is learned during training.\n",
    "Many pre-trained word embeddings are available (e.g., word2vec) but here we will learn it from scratch along with the rest of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding layer. We need to specify:\n",
    "# - the input dimension, that is, how many words in the vocabulary\n",
    "# - the embedding dimension, that is, how \"big\" is the word vectors space \n",
    "input_dim = len(SRC.vocab)\n",
    "embedding_dim = 32\n",
    "src_emb_layer = nn.Embedding(input_dim, embedding_dim)\n",
    "\n",
    "# Apply it to the example batch and display it\n",
    "embedded_batch = src_emb_layer(example_batch.src)\n",
    "print(embedded_batch)\n",
    "\n",
    "# The shape of the word vectors for a batch should be [seq_length, batch_size, embedding_dim]\n",
    "print(embedded_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "The core idea of a dropout layer is to reduce the risk of overfitting by randomly setting some inputed values at 0. Since the non-zero inputs (and the corresponding weights in the network) are not the same from one batch to another, it results in forcing these weights not to be batch-specific, and therefore avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the percentage of zeroed values (expressed between 0 and 1) is given as input\n",
    "dropout_layer = nn.Dropout(0.5)\n",
    "drop_batch = dropout_layer(embedded_batch)\n",
    "\n",
    "# in this example, half of the entries (50%) are set at 0\n",
    "print(drop_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent layers\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Rezzy-Caraka/publication/346410173/figure/fig2/AS:962598073823234@1606512673418/Network-Structure-of-RNN-LSTM-and-GRU.png\" width=\"500\"/>\n",
    "<center><a href=\"https://www.researchgate.net/profile/Rezzy-Caraka/publication/346410173_Employing_Long_Short-Term_Memory_and_Facebook_Prophet_Model_in_Air_Temperature_Forecasting/links/60077104a6fdccdcb868957f/Employing-Long-Short-Term-Memory-and-Facebook-Prophet-Model-in-Air-Temperature-Forecasting.pdf\">Source</a></center>\n",
    "\n",
    "We now see the 3 main recurrent layers (simple RNN, LSTM and GRU). We won't focused on the technical difference between these, but you can find more info online (e.g., [here](https://medium.com/analytics-vidhya/rnn-vs-gru-vs-lstm-863b0b7b1573)).\n",
    "\n",
    "#### Simple RNN\n",
    "\n",
    "First, let's see the basic RNN. We note $x_t$ the $t$-th element of the input to the RNN (in our case: this is the embedding after dropout). We have $h_t = \\text{RNN}(x_t, h_{t-1})$, where $h_{t}$ is the hidden state. To define such an RNN in Pytorch (using ```nn.RNN```), you need to specify:\n",
    "\n",
    "- the size of the input (here, it's the size of the embeddings)\n",
    "- the size of the hidden space (hidden_dim)\n",
    "- the number of layers (n_layers)\n",
    "\n",
    "By default, the RNN is uni-directional, uses bias, and uses tanh as activation function. If you use a multi-layer RNN, you can also add dropout in the intermediate layers. You can change these by playing with the parameters of the function (see the [doc](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN) for more info).\n",
    "\n",
    "**Note**: for the first element of the sequence, we have $h_1 = \\text{RNN}(x_1, h_{0})$, so we normally need to provide an initial hidden state $h_0$. In pytorch, we can either provide it explicitly or not. If we don't, it will use $h_0=0$ by default. This is what we do here, and it also applies to other recurrent units (LSTM and GRU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a basic RNN\n",
    "hidden_dim = 50\n",
    "n_layers = 2\n",
    "rnn = nn.RNN(embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "# Apply the RNN: you should give as input:\n",
    "# - the embeddeding after dropout calculated beforehand 'drop_batch'\n",
    "# The RNN returns:\n",
    "# - 'rnn_out': the whole sequence of hidden states of the last layer\n",
    "# - 'rnn_hidden': the final hidden states (for the last token) for all layers\n",
    "rnn_out, rnn_hidden = rnn(drop_batch)\n",
    "\n",
    "# Get the shape of the 'rnn_out': it should be [seq_length, batch_size, hidden_dim]\n",
    "print(rnn_out.shape)\n",
    "\n",
    "# Get the shape of the final hidden state 'rnn_hidden': it should be [n_layers, batch_size, hidden_dim]\n",
    "print(rnn_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write a 3-layer bidirectional RNN (check the doc!) and apply it to the same input ('embedded_batch')\n",
    "# Print the shape of rnn_out (it should be [seq_length, batch_size, 2*hidden_dim])\n",
    "# Print the shape of the final hidden state (it should be [2*n_layers, batch_size, hidden_dim])\n",
    "# (the factor '2' in the shapes comes from the fact that the network is bidirectional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "The basic RNN suffers from the gradient vanishing problem, so we will instead use a variant of it called *long short-term memory* (LSTM) networks. The key idea of LSTM is that it has an extra hidden feature called a *cell state* which allows the network to \"remember\" which part of the input sequence is useful or not, and therefore to avoid backpropagating the gradient throughout the whole sequence, thus avoiding gradient vanishing.\n",
    "\n",
    "The formula for the [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) is therefore: $(h_t, c_t) = \\text{LSTM}(x_t, h_{t-1}, c_{t-1})$ where $c_t$ is this extra cell state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an LSTM\n",
    "hidden_dim = 50\n",
    "n_layers = 2\n",
    "lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "# Apply the LSTM to the embedded batch\n",
    "lstm_out, (lstm_hidden, lstm_cell) = lstm(embedded_batch)\n",
    "\n",
    "# The shape of the output and final hidden state are the same as before.\n",
    "# The final cell state as the same size as the final hidden state.\n",
    "print(lstm_out.shape)\n",
    "print(lstm_hidden.shape)\n",
    "print(lstm_cell.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU\n",
    "\n",
    "The last main type of recurrent layer is the gated reccurent unit (GRU). You can see it as sort of a simplified LSTM: it also has some memory mechanism to avoid gradient vanishing, but it outputs only a single hidden state vector (instead of the additional cell state in LSTM). It generally performs similarly with LSTM (but this depends on applications). [Writting a GRU in pytorch](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU) is similar to a basic RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: using the doc, write a GRU layer with a hidden size of 50 and 2 layers.\n",
    "# Apply it to the embedded batch as before, and print the shape of the output and final hidden state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the translation model\n",
    "\n",
    "We'll now build the machine translation model. This model is based on two part:\n",
    "\n",
    "- an *encoder*, which takes as input the source sentence (in German) and encodes it into a *context* vector. This context vector is sort of a summary of the whole input sentence.\n",
    "- a *decoder*, which takes as input this context vector and sequentially generates a sentence in English. It always starts with the `<sos>` token and uses the context vector to generate the second token. Then, it recursively uses the last produced token and the hidden state to generate the next token.\n",
    "\n",
    "<img src=\"https://github.com/bentrevett/pytorch-seq2seq/raw/49df8404d938a6edbf729876405558cc2c2b3013//assets/seq2seq1.png\" />\n",
    "<center><a href=\"https://github.com/bentrevett/pytorch-seq2seq\">Source</a></center>\n",
    "<center>Note: On this picture, $h_t$ represent the hidden states of the encoder and $s_t$ the hidden states of the decoder. For LSTM, we also need to consider the cell states, but they're not displayed here for brevity.</center>\n",
    "\n",
    "### Encoder\n",
    "\n",
    "First, we write the encoder. It consists of:\n",
    "\n",
    "- an embedding layer to transform token indices into word vectors.\n",
    "- a dropout layer to reduce overfitting.\n",
    "- a 2-layer LSTM, to learn the context vector.\n",
    "\n",
    "**Note**: For the encoder, we don't need to keep track of all the hidden states ($h_1$, $h_2$, $h_3$, etc.), we only need the final hidden state called the context vector (and denoted $z$ on the picture). Therefore, we can simply apply our LSTM on the whole sequence, instead of writting a loop explicitly (this will not be the case for the decoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim_enc, n_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TO DO: store the parameters (reminder: use 'self.')\n",
    "        \n",
    "        # TO DO: create the embedding layer (transform indices into word vectors)\n",
    "        \n",
    "        # TO DO: create the dropout layer\n",
    "        \n",
    "        # TO DO: create the LSTM layer\n",
    "        \n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        # TO DO: first compute the embeddings\n",
    "        \n",
    "        # TO DO: apply dropout to the word embeddings\n",
    "        \n",
    "        # TO DO: apply the LSTM layer\n",
    "        \n",
    "        # TO DO: return the final hidden and cell states\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder parameters\n",
    "input_dim = len(SRC.vocab)\n",
    "embedding_dim_enc = 32\n",
    "hidden_dim_enc = 50\n",
    "n_layers = 1\n",
    "dropout_rate = 0.5\n",
    "\n",
    "# Instanciate the encoder and print the number of trainable parameters\n",
    "lstm_encoder = Encoder(input_dim, embedding_dim_enc, hidden_dim_enc, n_layers, dropout_rate)\n",
    "print('Number of parameters:', sum(p.numel() for p in lstm_encoder.parameters() if p.requires_grad))\n",
    "\n",
    "# Apply it to the example batch\n",
    "enc_hidden, enc_cell = lstm_encoder(example_batch.src)\n",
    "print(enc_hidden.shape)\n",
    "print(enc_cell.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Next, we'll build our decoder. Let us note that we treat machine translation as a classification task: the decoder tries to predict the probability of all token indices in the output (target) vocabulary from an input token index. This has two implications:\n",
    "\n",
    "- In addition to the embedding, dropout and LSTM layers, the decoder applies an extra linear layer/MLP to perform prediction of the probabilities. Therefore, this linear layer goes from a space of size `hidden_dim_dec` to a space of size `output_dim`, which is the number of tokens in the target vocabulary.\n",
    "- the decoder doesn't process all the sentence at once, but token by token, since the input at step $t$ is the word that has been predicted at step $t-1$ (not just the hidden state). Therefore, the input to the decoder has a sequence length of 1 (the recursive calculation over the whole sentence will be done in the full model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim_dec, n_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TO DO: store parameters\n",
    "        \n",
    "        # TO DO: create the embedding, dropout and LSTM layers\n",
    "        \n",
    "        # TO DO: create the linear layer (it goes from a space of size 'hidden_dim_dec' to 'output_dim')\n",
    "        \n",
    "    def forward(self, input_idx, input_hidden, input_cell):\n",
    "        \n",
    "        # apply the embedding and dropout layers\n",
    "        y = self.dropout_layer(self.embedding_layer(input_idx))\n",
    "        \n",
    "        # since y has a shape [batch_size, hidden_dim_dec], we need to unsqueeze it\n",
    "        # to create an artificial 'seq_length' (=1) dimension\n",
    "        y = y.unsqueeze(0)\n",
    "        \n",
    "        # TO DO: apply the lstm layer. Unlike the encoder, we need to store all the outputs to predict the target token\n",
    "        \n",
    "        # TO DO: squeeze 'output' (to remove the useless dimension 'seq_length'=1)\n",
    "        \n",
    "        # TO DO: apply the linear layer to predict the probabilities\n",
    "        \n",
    "        # TO DO: return the predicted probabilities per token, and the hidden / cell states of the LSTM decoder\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the decoder parameters\n",
    "output_dim = len(TRG.vocab)\n",
    "hidden_dim_dec = 50\n",
    "embedding_dim_dec = 32\n",
    "\n",
    "# Instanciate the decoder and print the number of trainable parameters\n",
    "lstm_decoder = Decoder(output_dim, embedding_dim_dec, hidden_dim_dec, n_layers, dropout_rate)\n",
    "print('Number of parameters:', sum(p.numel() for p in lstm_decoder.parameters() if p.requires_grad))\n",
    "\n",
    "# Create an artificial numerized input token (input indices) with value '2'\n",
    "# (it corresponds to the 'start of sentence' or '<sos>' token)\n",
    "input_idx = torch.ones(batch_size).int() * 2\n",
    "\n",
    "# Apply the decoder\n",
    "pred_proba, dec_hidden, dec_cell = lstm_decoder(input_idx, enc_hidden, enc_cell)\n",
    "print(pred_proba.shape)\n",
    "print(dec_hidden.shape)\n",
    "print(dec_cell.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: from the predicted probabilities calculated before 'pred_proba', get the index with the highest probability\n",
    "\n",
    "# TO DO: for each element in the batch, transform this index back to an actual token (word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full model\n",
    "\n",
    "Finally, we need to implement the overall model, which takes an input sentence, produces the context vectors using the encoder, and produces the output sentence recursively using the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store the encoder, decoder, and the target vocabulary size\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.trg_vocab_size = decoder.output_dim\n",
    "        \n",
    "    def forward(self, src, trg_len):\n",
    "        \n",
    "        # The maximum length of the target sentences in the batch is given as input 'trg_len'\n",
    "        \n",
    "        # Create a tensor to store the predicted probabilities from the decoder\n",
    "        batch_size = src.shape[-1]\n",
    "        pred_probas = torch.zeros(trg_len, batch_size, self.trg_vocab_size)\n",
    "        \n",
    "        # Give a probability of 1 to the element correponding to <sos> (index=2) for the first element of all sequences in the batch \n",
    "        pred_probas[0, :, 2] = 1\n",
    "\n",
    "        # Create the first input to the decoder is the <sos> token (coded by '2' in our vocabulary)\n",
    "        input_idx = torch.ones(batch_size).int() * 2\n",
    "        \n",
    "        # TO DO: apply the encoder to the src sentence and get the last hidden and cell states (=context vectors)\n",
    "\n",
    "        \n",
    "        # loop over tokens: it should start from 1 (not 0) since the very first token is already known (<sos>)\n",
    "        for t in range(1, trg_len):\n",
    "\n",
    "            # TO DO: apply the decoder using the current input_idx, hidden and cell states\n",
    "            # It should output the predicted proba at step t (let's call it 'pred_proba_t',\n",
    "            # and the updated hidden and cell states\n",
    "            \n",
    "            # TO DO: store this current pred_proba_t in the tensor pred_probas\n",
    "            # It will be used to compute the classification loss during training\n",
    "            \n",
    "            # TO DO: from 'pred_proba_t', get the index with the highest probability and use it as the next input\n",
    "        \n",
    "        return pred_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the model and print the number of parameters (should be exactly params(encoder)+params(decoder))\n",
    "model = Seq2Seq(lstm_encoder, lstm_decoder)\n",
    "print('Number of parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "# TO DO: Apply the model to the 'example_batch'\n",
    "\n",
    "# TO DO: Get the indices of highest predicted \n",
    "\n",
    "# TO DO: display one of the predicted sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now we have our model implemented, we can train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training function is very similar to what has been done before, except a few manipulation before computing the loss\n",
    "def training_seq2seq(model, train_dataloader, num_epochs, loss_fn, optimizer, device='cpu', verbose=True):\n",
    "\n",
    "    # Set the model in 'training' mode\n",
    "    model.train()\n",
    "\n",
    "    # Copy the model to device\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize a list to save the training loss over epochs\n",
    "    loss_total = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        loss_current_epoch = 0\n",
    "\n",
    "        # loop over batches\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Get the source and target sentence, and the target length, copy it to device\n",
    "            src, trg = batch.src.to(device), batch.trg.to(device)\n",
    "            trg_len = trg.shape[0]\n",
    "\n",
    "            # Set the gradients at 0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Apply the model\n",
    "            pred_probas = model(src, trg_len)\n",
    "\n",
    "            # Remove the first token (always <sos>) to compute the loss\n",
    "            output_dim = pred_probas.shape[-1]\n",
    "            pred_probas = pred_probas[1:]\n",
    "\n",
    "            # Reshape the pred_probas and target so that they have appropriate shapes:\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "            pred_probas = pred_probas.view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            # Compute the loss and the gradients\n",
    "            loss = loss_fn(pred_probas, trg)\n",
    "            loss.backward()\n",
    "\n",
    "            # backpropagation (clip the gradients at a maximum value 1 to avoid too large steps)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record the loss\n",
    "            loss_current_epoch += loss.item()\n",
    "\n",
    "        # At the end of each epoch, save the average loss over batches and display it\n",
    "        loss_total.append(loss_current_epoch)\n",
    "        if verbose:\n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss_current_epoch))\n",
    "\n",
    "        # Save the trained over epochs (in case it's interrupted for some reason)\n",
    "        torch.save(model.state_dict(), 'lstm_model.pt')\n",
    "        \n",
    "    return loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer (ADAM), set the number of epochs\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "num_epochs = 10\n",
    "\n",
    "# For the loss function, since we treat the problem as a classification task, we use the cross entropy.\n",
    "# We also tell it to ignore the index of the <pad> token for computation speed\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "\n",
    "# Apply the training function\n",
    "loss_total = training_seq2seq(model, train_dataloader, num_epochs, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now the model is trained, we can evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The evaluation function is similar to what was done in previous labs.\n",
    "def evaluate_seq2seq(model, eval_dataloader, loss_fn, device='cpu', verbose=True):\n",
    "\n",
    "    # Set the model in 'eval' mode (disable dropout layer)\n",
    "    model.eval()\n",
    "\n",
    "    # Copy the model to device\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize the eval loss\n",
    "    loss_eval = 0\n",
    "\n",
    "    # loop over batches\n",
    "    for i, batch in enumerate(eval_dataloader):\n",
    "\n",
    "        # Get the source and target sentence, and the target length, copy it to device\n",
    "        src, trg = batch.src.to(device), batch.trg.to(device)\n",
    "        trg_len = trg.shape[0]\n",
    "\n",
    "        # Apply the model\n",
    "        pred_probas = model(src, trg_len)\n",
    "\n",
    "        # Remove the first token (always <sos>) to compute the loss\n",
    "        output_dim = pred_probas.shape[-1]\n",
    "        pred_probas = pred_probas[1:]\n",
    "\n",
    "        # Reshape the pred_probas and target so that they have appropriate shapes:\n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        pred_probas = pred_probas.view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(pred_probas, trg)\n",
    "\n",
    "        # Record the loss\n",
    "        loss_eval += loss.item()\n",
    "\n",
    "    return loss_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Instanciate a model and load the trained parameters\n",
    "\n",
    "# TO DO: Evalute it on the test set and display the loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some examples (source and target) in the test set\n",
    "example_batch_src, example_batch_trg = example_batch.src, example_batch.trg\n",
    "\n",
    "# TO DO: Compute predictions with the model\n",
    "\n",
    "# TO DO: Print the true and predicted target sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see on the example above, the results are not very good. There are many strategies to improve performance:\n",
    "\n",
    "- *skip-filtering*, which means feeding each RNN cell with the whole context vector instead of just the previous hidden state (done [here](https://github.com/bentrevett/pytorch-seq2seq/blob/master/2%20-%20Learning%20Phrase%20Representations%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation.ipynb)).\n",
    "- *teacher forcing* in the decoder at training, which means using the ground truth token as input to each decoder cell instead of the predicted token from the previous cell (done [here](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb)).\n",
    "- *packed padded sentences* with masking, which allows to skip the `<pad>` token in the encoder and save time (done [here](https://github.com/bentrevett/pytorch-seq2seq/blob/master/4%20-%20Packed%20Padded%20Sequences%2C%20Masking%2C%20Inference%20and%20BLEU.ipynb)).\n",
    "\n",
    "There are also more simple changes we can do readily. These are implemented in the next script (bonus work)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
