{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading, displaying, and preprocessing \n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from skimage import io, transform\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import soundfile as sf\n",
    "import fnmatch\n",
    "from IPython.display import Audio, display\n",
    "from matplotlib import pyplot as plt\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE data - load and display one image\n",
    "image_example = io.imread('data/image_corpus/0001.jpg')\n",
    "plt.imshow(image_example)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by creating a class object for representing our image data\n",
    "# This is a subclass of torch.utils.data.dataset.Dataset that will serve as input to the DataLoader\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_dir):\n",
    "        \"\"\"Here we initialize the attributes of the object of the class.\"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = sorted(self._find_files(image_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Here we return the size of the dataset.\"\"\"\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Here we return a data sample for a given index.\"\"\"\n",
    "        # TO DO: write this function: it takes as input 'index' (which is an integer number),\n",
    "        # and returns the corresponding item as a pytorch tensor\n",
    "        # hint: use 'self.image_files' , which is the list of images paths defined in the '__init__' method\n",
    "        return x\n",
    "\n",
    "    def _find_files(self, directory, pattern='*.jpg'):\n",
    "        \"\"\"Recursively finds all files matching the pattern.\"\"\"\n",
    "        files = []\n",
    "        for root, dirnames, filenames in os.walk(directory):\n",
    "            for filename in fnmatch.filter(filenames, pattern):\n",
    "                files.append(os.path.join(root, filename))\n",
    "        return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can initialize the dataset by providing the directory to the image corpus data\n",
    "image_dir = 'data/image_corpus/'\n",
    "dataset = ImageDataset(image_dir=image_dir)\n",
    "\n",
    "# You can use the '_find_files' method to get the list of images paths\n",
    "images_list = dataset._find_files(image_dir)\n",
    "for l in images_list:\n",
    "    print(l)\n",
    "print(len(images_list))\n",
    "\n",
    "# To get the length of the dataset, you can use the '__len__' method or 'len()' directly\n",
    "print(dataset.__len__())\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: load the image with index '0' from the dataset, print its shape and display it\n",
    "# hint: you can use either '__getitem__(index)', or more simply '[index]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can create a dataloader, which samples over the dataset and generates small batches of images\n",
    "batch_size = 2\n",
    "image_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate over the dataset to access each batch, and display the images\n",
    "for x in image_dataloader:\n",
    "    print(x.shape)\n",
    "    x = x.int().numpy()\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(x[0])\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(x[1])\n",
    "    \n",
    "    plt.show()\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: create a dataloader with a batch size of 5 and iterate over the dataloader to display the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio data: a dataset for storing the speech data.\n",
    "# TO DO: implement the __init__, __len__ and __getitem__ methods for the SpeechDataset class\n",
    "\n",
    "class SpeechDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_dir, sample_rate, max_sig_length):\n",
    "        # create variables for storing the attributes of the class\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the length of the list of audio files\n",
    "        return \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # create a <filename> variable and store the index-th audio file in it\n",
    "        \n",
    "        # get the file name, and read it (use 'sf.read(filename)'') and store the signal in a tensor\n",
    "        \n",
    "        # crop x so it has a length equal to 'max_sig_length' (hint: use slicing)\n",
    "\n",
    "        # check if the sample rates match\n",
    "        assert original_sample_rate == self.sample_rate\n",
    "        \n",
    "        # reshape so that each audio has shape (num_samples, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _find_files(self, directory, pattern='*.wav'):\n",
    "        # Recursively finds all files matching the pattern.\n",
    "        files = []\n",
    "        for root, dirnames, filenames in os.walk(directory):\n",
    "            for filename in fnmatch.filter(filenames, pattern):\n",
    "                files.append(os.path.join(root, filename))\n",
    "        return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can initialize the dataset by providing the directory to the audio corpus data, the target sample rate\n",
    "# and the maximum signal length (to avoid signals of different lengths)\n",
    "audio_dir = 'data/audio_corpus/'\n",
    "sample_rate = 16000\n",
    "max_sig_length = 16000\n",
    "dataset = SpeechDataset(audio_dir=audio_dir, sample_rate=sample_rate, max_sig_length=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an audio excerpt from the dataset\n",
    "x = dataset[0]\n",
    "print(x.shape)\n",
    "\n",
    "# To display audio files, we can use 'display(Audio(data, sample_rate))' to have a graphical audio reader\n",
    "plt.plot(x.numpy())\n",
    "plt.show()\n",
    "display(Audio(data=x.numpy(), rate=sample_rate))\n",
    "\n",
    "# Get the list of files and it's length\n",
    "audio_list = dataset._find_files(audio_dir)\n",
    "for l in audio_list:\n",
    "    print(l)\n",
    "\n",
    "# Get the length of the dataset\n",
    "print('Dataset length :', len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we can create a dataloader, which samples over the dataset and generates small batches of audio\n",
    "batch_size = 3\n",
    "audio_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# TO DO: as for images, iterate over the dataloader to plot the audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: concatenate all the audio files into one single tensor with a dynamic compression\n",
    "\n",
    "# define a dataloader with batch_size=1\n",
    "\n",
    "# define an <energy_target> variable (=10) which controls the energy of each signal\n",
    "\n",
    "# initialize an empty tensor to concatenate everything\n",
    "\n",
    "# iterate over the dataloader\n",
    "for x in audio_dataloader:\n",
    "    # get the signal (only 1 per batch)\n",
    "    \n",
    "    # normalize it such that sqrt(sum x^2) = energy_target\n",
    "    \n",
    "    # concatenate it to the others in x_tot\n",
    "\n",
    "# remove the first element of x_tot (which is empty)\n",
    "\n",
    "# save it as a .pt file and as .wav audio (use sf.write)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Text data - An example for reading loading and displaying text data\n",
    "filepath = 'data/text_corpus/file_1.txt'\n",
    "for line in open(filepath, 'r'):\n",
    "    text, label = line.strip().split('\\t')\n",
    "print(\"text: {sentence}\\t Label: {label}\".format(sentence=text, label=label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for storing the text data.\n",
    "# TO DO: implement the __init__, __len__ and __getitem__ methods for the TextDataset class\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, text_dir, max_seq_length=128):\n",
    "        # create variables for storing the attributes of the class (text_dir, max_seq_length, and the list of text files)\n",
    "\n",
    "        # extract the labels from the given texts using _get_labels()\n",
    "\n",
    "        # create a vocabulary of unique words from the given text files using _create_vobabulary()\n",
    "\n",
    "        # create a word-to-index mapping, i.e., assign a unique (positive) integer to each word in the vocabulary\n",
    "        # hint: create a dictionary <word_to_index> which takes words as keys and unique integers (0, 1, 2, ...) as values\n",
    "\n",
    "        # special token [PAD] used for padding text to a fixed length (check _preprocess_text() for details)\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the length of the list of text files\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # get the index-th text file from the list of text files defined in __init__\n",
    "\n",
    "        # return a list of all tokens in the text and the respective label (use the _tokenize_text method)\n",
    "\n",
    "        # use the word_to_index mapping to transform the tokens into indices and save them into an IntTensor\n",
    "        # hint: store all indices in a list and then cast the list into a IntTensor\n",
    "\n",
    "        # get the index-th label and store it into a FloatTensor\n",
    "        y = torch.FloatTensor([self.labels[index]])\n",
    "\n",
    "        # stores the text indices and the label into a dictionary\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def _find_files(self, directory, pattern='*.txt'):\n",
    "        \"\"\"Recursively finds all files matching the pattern.\"\"\"\n",
    "        files = []\n",
    "        for root, dirnames, filenames in os.walk(directory):\n",
    "            for filename in fnmatch.filter(filenames, pattern):\n",
    "                files.append(os.path.join(root, filename))\n",
    "        return files\n",
    "\n",
    "    def _get_labels(self):\n",
    "        \"\"\"Extracts the labels from the given text files.\"\"\"\n",
    "        labels = []\n",
    "        for filepath in self.text_files:\n",
    "            text, label = list(open(filepath, 'r'))[0].split('\\t')\n",
    "            labels.append(int(label))\n",
    "        return labels\n",
    "\n",
    "    def _create_vocabulary(self):\n",
    "        \"\"\"Creates a vocabulary of unique words from the given text files.\"\"\"\n",
    "        all_texts = [list(open(filepath, 'r'))[0].strip().lower() for filepath in self.text_files]\n",
    "        letters = string.ascii_lowercase\n",
    "        word_string = ' '.join(all_texts)\n",
    "        not_letters = set([char for char in word_string if char not in letters and char != ' '])\n",
    "        for char in not_letters:\n",
    "            word_string = word_string.replace(char, \" \")\n",
    "        vocab = set(word_string.split())\n",
    "        return list(vocab)\n",
    "\n",
    "    def _tokenize_text(self, text_file):\n",
    "        \"\"\"\n",
    "        Removes non-characters from the text and pads the text to max_seq_len.\n",
    "        *!* Padding is necessary for ensuring that all text_files have the same size\n",
    "        *!* This is required since DataLoader cannot handle tensors of variable length\n",
    "\n",
    "        Returns a list of all tokens in the text\n",
    "        \"\"\"\n",
    "        text = list(open(text_file, 'r'))[0].strip().lower()\n",
    "        letters = string.ascii_lowercase\n",
    "        not_letters = set([char_ for char_ in text if char_ not in letters and char_ != ' '])\n",
    "        for char in not_letters:\n",
    "            text = text.replace(char, \" \")\n",
    "        tokens = text.split()\n",
    "        for i in range(self.max_seq_len - len(tokens)):\n",
    "            tokens.append('[PAD]')\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dir = 'data/text_corpus/'\n",
    "dataset = TextDataset(text_dir)\n",
    "\n",
    "# TO DO: create a dataloader for the text data and print the contents of each batch\n",
    "\n",
    "# define a dataloader with batch_size=2 and shuffle=True\n",
    "\n",
    "# iterate over the dataloader and store/print each feature (token_ids, label) separately\n",
    "# hint: check the output of __getitem__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
